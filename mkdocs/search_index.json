{
    "docs": [
        {
            "location": "/",
            "text": "OpenFace \n\n\n\n\nFree and open source face recognition with\ndeep neural networks.\n\n\n\n\n\n\nOpenFace is a Python and \nTorch\n implementation of\nface recognition with deep neural networks and is based on\nthe CVPR 2015 paper\n\nFaceNet: A Unified Embedding for Face Recognition and Clustering\n\nby Florian Schroff, Dmitry Kalenichenko, and James Philbin at Google.\nTorch allows the network to be executed on a CPU or with CUDA.\n\n\nCrafted by \nBrandon Amos\n in the\n\nElijah\n research group at\nCarnegie Mellon University.\n\n\n\n\n\n\nThe code is available on GitHub at\n  \ncmusatyalab/openface\n.\n\n\nJoin the\n  \ncmu-openface group\n\n  or the\n  \ngitter chat\n\n  for discussions and installation issues.\n\n\nDevelopment discussions and bugs reports are on the\n  \nissue tracker\n.\n\n\n\n\n\n\nThis research was supported by the National Science Foundation (NSF)\nunder grant number CNS-1518865.  Additional support\nwas provided by the Intel Corporation, Google, Vodafone, NVIDIA, and the\nConklin Kistler family fund.  Any opinions, findings, conclusions or\nrecommendations expressed in this material are those of the authors\nand should not be attributed to their employers or funding sources.\n\n\n\n\nIsn't face recognition a solved problem?\n\n\nNo! Accuracies from research papers have just begun to surpass\nhuman accuracies on some benchmarks.\nThe accuracies of open source face recognition systems lag\nbehind the state-of-the-art.\nSee \nour accuracy comparisons\n\non the famous LFW benchmark.\n\n\n\n\nPlease use responsibly!\n\n\nWe do not support the use of this project in applications\nthat violate privacy and security.\nWe are using this to help cognitively impaired users to\nsense and understand the world around them.\n\n\n\n\nOverview\n\n\nThe following overview shows the workflow for a single input\nimage of Sylvestor Stallone from the publicly available\n\nLFW dataset\n.\n\n\n\n\nDetect faces with a pre-trained models from\n  \ndlib\n\n  or\n  \nOpenCV\n.\n\n\nTransform the face for the neural network.\n   This repository uses dlib's\n   \nreal-time pose estimation\n\n   with OpenCV's\n   \naffine transformation\n\n   to try to make the eyes and bottom lip appear in\n   the same location on each image.\n\n\nUse a deep neural network to represent (or embed) the face on\n   a 128-dimensional unit hypersphere.\n   The embedding is a generic representation for anybody's face.\n   Unlike other face representations, this embedding has the nice property\n   that a larger distance between two face embeddings means\n   that the faces are likely not of the same person.\n   This property makes clustering, similarity detection,\n   and classification tasks easier than other face recognition\n   techniques where the Euclidean distance between\n   features is not meaningful.\n\n\nApply your favorite clustering or classification techniques\n   to the features to complete your recognition task.\n   See below for our examples for classification and\n   similarity detection, including an online web demo.\n\n\n\n\n\n\nCitations\n\n\nThe following is a \nBibTeX\n\nand plaintext reference\nfor the OpenFace GitHub repository.\nThe reference may change in the future.\nThe BibTeX entry requires the \nurl\n LaTeX package.\n\n\n@misc{amos2015openface,\n    title        = {{OpenFace: Face Recognition with Deep Neural Networks}},\n    author       = {Amos, Brandon and Harkes, Jan and Pillai, Padmanabhan and Elgazzar, Khalid and Satyanarayanan, Mahadev},\n    howpublished = {\\url{http://github.com/cmusatyalab/openface}},\n    note         = {Accessed: 2015-11-11}\n}\n\nBrandon Amos, Jan Harkes, Padmanabhan Pillai, Khalid Elgazzar,\nand Mahadev Satyanarayanan.\nOpenFace: Face Recognition with Deep Neural Networks.\nhttp://github.com/cmusatyalab/openface.\nAccessed: 2015-11-11.\n\n\n\n\nAcknowledgements\n\n\n\n\nThe fantastic Torch ecosystem and community.\n\n\nAlfredo Canziani's\n\n  implementation of FaceNet's loss function in\n  \ntorch-TripletEmbedding\n.\n\n\nNicholas L\u00e9onard\n\n  for quickly merging my pull requests to\n  \nnicholas-leonard/dpnn\n\n  modifying the inception layer.\n\n\nFrancisco Massa\n\n  and\n  \nAndrej Karpathy\n\n  for\n  quickly releasing \nnn.Normalize\n\n  after I expressed interest in using it.\n\n\nSoumith Chintala\n for\n  help with the \nfbcunn\n\n  example code.\n\n\nDavis King's\n \ndlib\n\n  library for face detection and alignment.\n\n\nZhuo Chen, Kiryong Ha, Wenlu Hu,\n  \nRahul Sukthankar\n, and\n  Junjue Wang for insightful discussions.\n\n\n\n\nLicensing\n\n\nThe source code and trained models \nnn4.v1.t7\n and\n\nceleb-classifier.nn4.v1.t7\n are copyright\nCarnegie Mellon University and licensed under the\n\nApache 2.0 License\n.\nPortions from the following third party sources have\nbeen modified and are included in this repository.\nThese portions are noted in the source files and are\ncopyright their respective authors with\nthe licenses listed.\n\n\n\n\n\n\n\n\nProject\n\n\nModified\n\n\nLicense\n\n\n\n\n\n\n\n\n\n\n\n\nAtcold/torch-TripletEmbedding\n\n\nNo\n\n\nMIT\n\n\n\n\n\n\n\n\nfacebook/fbnn\n\n\nYes\n\n\nBSD",
            "title": "Home"
        },
        {
            "location": "/#openface",
            "text": "Free and open source face recognition with\ndeep neural networks.    OpenFace is a Python and  Torch  implementation of\nface recognition with deep neural networks and is based on\nthe CVPR 2015 paper FaceNet: A Unified Embedding for Face Recognition and Clustering \nby Florian Schroff, Dmitry Kalenichenko, and James Philbin at Google.\nTorch allows the network to be executed on a CPU or with CUDA.  Crafted by  Brandon Amos  in the Elijah  research group at\nCarnegie Mellon University.    The code is available on GitHub at\n   cmusatyalab/openface .  Join the\n   cmu-openface group \n  or the\n   gitter chat \n  for discussions and installation issues.  Development discussions and bugs reports are on the\n   issue tracker .    This research was supported by the National Science Foundation (NSF)\nunder grant number CNS-1518865.  Additional support\nwas provided by the Intel Corporation, Google, Vodafone, NVIDIA, and the\nConklin Kistler family fund.  Any opinions, findings, conclusions or\nrecommendations expressed in this material are those of the authors\nand should not be attributed to their employers or funding sources.   Isn't face recognition a solved problem?  No! Accuracies from research papers have just begun to surpass\nhuman accuracies on some benchmarks.\nThe accuracies of open source face recognition systems lag\nbehind the state-of-the-art.\nSee  our accuracy comparisons \non the famous LFW benchmark.   Please use responsibly!  We do not support the use of this project in applications\nthat violate privacy and security.\nWe are using this to help cognitively impaired users to\nsense and understand the world around them.",
            "title": "OpenFace "
        },
        {
            "location": "/#overview",
            "text": "The following overview shows the workflow for a single input\nimage of Sylvestor Stallone from the publicly available LFW dataset .   Detect faces with a pre-trained models from\n   dlib \n  or\n   OpenCV .  Transform the face for the neural network.\n   This repository uses dlib's\n    real-time pose estimation \n   with OpenCV's\n    affine transformation \n   to try to make the eyes and bottom lip appear in\n   the same location on each image.  Use a deep neural network to represent (or embed) the face on\n   a 128-dimensional unit hypersphere.\n   The embedding is a generic representation for anybody's face.\n   Unlike other face representations, this embedding has the nice property\n   that a larger distance between two face embeddings means\n   that the faces are likely not of the same person.\n   This property makes clustering, similarity detection,\n   and classification tasks easier than other face recognition\n   techniques where the Euclidean distance between\n   features is not meaningful.  Apply your favorite clustering or classification techniques\n   to the features to complete your recognition task.\n   See below for our examples for classification and\n   similarity detection, including an online web demo.",
            "title": "Overview"
        },
        {
            "location": "/#citations",
            "text": "The following is a  BibTeX \nand plaintext reference\nfor the OpenFace GitHub repository.\nThe reference may change in the future.\nThe BibTeX entry requires the  url  LaTeX package.  @misc{amos2015openface,\n    title        = {{OpenFace: Face Recognition with Deep Neural Networks}},\n    author       = {Amos, Brandon and Harkes, Jan and Pillai, Padmanabhan and Elgazzar, Khalid and Satyanarayanan, Mahadev},\n    howpublished = {\\url{http://github.com/cmusatyalab/openface}},\n    note         = {Accessed: 2015-11-11}\n}\n\nBrandon Amos, Jan Harkes, Padmanabhan Pillai, Khalid Elgazzar,\nand Mahadev Satyanarayanan.\nOpenFace: Face Recognition with Deep Neural Networks.\nhttp://github.com/cmusatyalab/openface.\nAccessed: 2015-11-11.",
            "title": "Citations"
        },
        {
            "location": "/#acknowledgements",
            "text": "The fantastic Torch ecosystem and community.  Alfredo Canziani's \n  implementation of FaceNet's loss function in\n   torch-TripletEmbedding .  Nicholas L\u00e9onard \n  for quickly merging my pull requests to\n   nicholas-leonard/dpnn \n  modifying the inception layer.  Francisco Massa \n  and\n   Andrej Karpathy \n  for\n  quickly releasing  nn.Normalize \n  after I expressed interest in using it.  Soumith Chintala  for\n  help with the  fbcunn \n  example code.  Davis King's   dlib \n  library for face detection and alignment.  Zhuo Chen, Kiryong Ha, Wenlu Hu,\n   Rahul Sukthankar , and\n  Junjue Wang for insightful discussions.",
            "title": "Acknowledgements"
        },
        {
            "location": "/#licensing",
            "text": "The source code and trained models  nn4.v1.t7  and celeb-classifier.nn4.v1.t7  are copyright\nCarnegie Mellon University and licensed under the Apache 2.0 License .\nPortions from the following third party sources have\nbeen modified and are included in this repository.\nThese portions are noted in the source files and are\ncopyright their respective authors with\nthe licenses listed.     Project  Modified  License       Atcold/torch-TripletEmbedding  No  MIT     facebook/fbnn  Yes  BSD",
            "title": "Licensing"
        },
        {
            "location": "/demo-1-web/",
            "text": "Demo 1: Real-Time Web Demo\n\n\nSee \nour YouTube video\n\nof using this in a real-time web application\nfor face recognition.\nThe source is available in\n\ndemos/web\n.\nThe browser portions have been tested on Google Chrome 46 in OSX.\n\n\n\n\nThis demo does the full face recognition pipeline on every frame.\nIn practice, object tracking\n\nlike dlib's\n\nshould be used once the face recognizer has predicted a face.\n\n\nIn the edge case when a single person is trained,\nthe classifier has no knowledge of other people and\nlabels anybody with the name of the trained person.\n\n\nThe web demo does not predict unknown users.\nIf you're interested in predicting unknown people,\none idea is to use a probabilistic classifier to predict\nconfidence scores and then call the prediction unknown\nif the confidence is too low.\nSee the \nclassification demo\n\nfor an example of using a probabilistic classifier.\n\n\n\n\nSetup\n\n\nTo run on your system, first follow the\n\nSetup Guide\n and make sure you can\nrun a simpler demo, like the \ncomparison demo\n.\n\n\nNext, install the requirements for the web demo with\n\n./install-deps.sh\n and \nsudo pip install -r requirements.txt\n\nfrom the \ndemos/web\n directory.\nThis is currently not included in the Docker container.\nThe application is split into a processing server and static\nweb pages that communicate via web sockets.\n\n\nStart the server with \n./demos/web/server.py\n.\nWith your client system with webcam and browser,\nyou should now be able to send a request to the websocket\nconnection with \ncurl your-server:9000\n (\nlocalhost:9000\n if running on your machine),\nwhich should inform you that it's' a WebSocket endpoint and not a web server.\nPlease check routing between your client and server if you\nget connection refused issues.\n\n\nIf you are running the server remotely (relative to your browser)\nor in a Docker container,\nchange the WebSocket connection in\n\nindex.html\n\nfrom \n127.0.0.1\n to the IP address of your server\nthat you were able to connect to with \ncurl\n.\nWith the WebSocket server running, serve the static website with\n\npython2 -m SimpleHTTPServer 8000\n from the \n/demos/web\n directory.\nYou should now be able to access the demo from your browser\nat \nhttp://your-server:8000\n, (\nhttp://localhost:8000\n if running on your machine),\nThe saved faces are only available for the browser session.",
            "title": "Demo 1 - Real-time Web"
        },
        {
            "location": "/demo-1-web/#demo-1-real-time-web-demo",
            "text": "See  our YouTube video \nof using this in a real-time web application\nfor face recognition.\nThe source is available in demos/web .\nThe browser portions have been tested on Google Chrome 46 in OSX.   This demo does the full face recognition pipeline on every frame.\nIn practice, object tracking like dlib's \nshould be used once the face recognizer has predicted a face.  In the edge case when a single person is trained,\nthe classifier has no knowledge of other people and\nlabels anybody with the name of the trained person.  The web demo does not predict unknown users.\nIf you're interested in predicting unknown people,\none idea is to use a probabilistic classifier to predict\nconfidence scores and then call the prediction unknown\nif the confidence is too low.\nSee the  classification demo \nfor an example of using a probabilistic classifier.",
            "title": "Demo 1: Real-Time Web Demo"
        },
        {
            "location": "/demo-1-web/#setup",
            "text": "To run on your system, first follow the Setup Guide  and make sure you can\nrun a simpler demo, like the  comparison demo .  Next, install the requirements for the web demo with ./install-deps.sh  and  sudo pip install -r requirements.txt \nfrom the  demos/web  directory.\nThis is currently not included in the Docker container.\nThe application is split into a processing server and static\nweb pages that communicate via web sockets.  Start the server with  ./demos/web/server.py .\nWith your client system with webcam and browser,\nyou should now be able to send a request to the websocket\nconnection with  curl your-server:9000  ( localhost:9000  if running on your machine),\nwhich should inform you that it's' a WebSocket endpoint and not a web server.\nPlease check routing between your client and server if you\nget connection refused issues.  If you are running the server remotely (relative to your browser)\nor in a Docker container,\nchange the WebSocket connection in index.html \nfrom  127.0.0.1  to the IP address of your server\nthat you were able to connect to with  curl .\nWith the WebSocket server running, serve the static website with python2 -m SimpleHTTPServer 8000  from the  /demos/web  directory.\nYou should now be able to access the demo from your browser\nat  http://your-server:8000 , ( http://localhost:8000  if running on your machine),\nThe saved faces are only available for the browser session.",
            "title": "Setup"
        },
        {
            "location": "/demo-2-comparison/",
            "text": "Demo 2: Comparing two images\n\n\nThe \ncomparison demo\n outputs the predicted similarity\nscore of two faces by computing the squared L2 distance between\ntheir representations.\nA lower score indicates two faces are more likely of the same person.\nSince the representations are on the unit hypersphere, the\nscores range from 0 (the same picture) to 4.0.\nThe following distances between images of John Lennon and\nEric Clapton were generated with\n\n./demos/compare.py images/examples/{lennon*,clapton*}\n.\n\n\n\n\n\n\n\n\nLennon 1\n\n\nLennon 2\n\n\nClapton 1\n\n\nClapton 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following table shows that a distance threshold of \n0.8\n would\ndistinguish these two people.\nIn practice, further experimentation should be done on the distance threshold.\nOn our LFW experiments, the mean threshold across multiple\nexperiments is about 0.68,\nsee \naccuracies.txt\n.\n\n\n\n\n\n\n\n\nImage 1\n\n\nImage 2\n\n\nDistance\n\n\n\n\n\n\n\n\n\n\nLennon 1\n\n\nLennon 2\n\n\n0.298\n\n\n\n\n\n\nLennon 1\n\n\nClapton 1\n\n\n0.985\n\n\n\n\n\n\nLennon 1\n\n\nClapton 2\n\n\n0.855\n\n\n\n\n\n\nLennon 2\n\n\nClapton 1\n\n\n0.904\n\n\n\n\n\n\nLennon 2\n\n\nClapton 2\n\n\n0.955\n\n\n\n\n\n\nClapton 1\n\n\nClapton 2\n\n\n0.712",
            "title": "Demo 2 - Comparison"
        },
        {
            "location": "/demo-2-comparison/#demo-2-comparing-two-images",
            "text": "The  comparison demo  outputs the predicted similarity\nscore of two faces by computing the squared L2 distance between\ntheir representations.\nA lower score indicates two faces are more likely of the same person.\nSince the representations are on the unit hypersphere, the\nscores range from 0 (the same picture) to 4.0.\nThe following distances between images of John Lennon and\nEric Clapton were generated with ./demos/compare.py images/examples/{lennon*,clapton*} .     Lennon 1  Lennon 2  Clapton 1  Clapton 2             The following table shows that a distance threshold of  0.8  would\ndistinguish these two people.\nIn practice, further experimentation should be done on the distance threshold.\nOn our LFW experiments, the mean threshold across multiple\nexperiments is about 0.68,\nsee  accuracies.txt .     Image 1  Image 2  Distance      Lennon 1  Lennon 2  0.298    Lennon 1  Clapton 1  0.985    Lennon 1  Clapton 2  0.855    Lennon 2  Clapton 1  0.904    Lennon 2  Clapton 2  0.955    Clapton 1  Clapton 2  0.712",
            "title": "Demo 2: Comparing two images"
        },
        {
            "location": "/demo-3-classifier/",
            "text": "Demo 3: Training a Classifier\n\n\nOpenFace's core provides a feature extraction method to\nobtain a low-dimensional representation of any face.\n\ndemos/classifier.py\n\nshows a demo of how these representations can be\nused to create a face classifier.\n\n\nThere is a distinction between training the deep neural network (DNN)\nmodel for feature representation\nand training a model for classifying people with the DNN model.\nThis shows how to use a pre-trained DNN model to train and use\na classification model.\n\n\nCreating a Classification Model\n\n\n1. Create raw image directory.\n\n\nCreate a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as \njpg\n or \npng\n and have\na lowercase extension.\n\n\n$ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png\n\n\n\n\n2. Preprocess the raw images\n\n\nChange \n8\n to however many\nseparate processes you want to run:\n\nfor N in {1..8}; do ./util/align-dlib.py \npath-to-raw-data\n align innerEyesAndBottomLip \npath-to-aligned-data\n --size 96 \n done\n.\n\n\n3. Generate Representations\n\n\n./batch-represent/main.lua -outDir \nfeature-directory\n -data \npath-to-aligned-data\n\ncreates \nreps.csv\n and \nlabels.csv\n in \nfeature-directory\n.\n\n\n4. Create the Classification Model\n\n\nUse \n./demos/classifier.py train \nfeature-directory\n to produce\nthe classification model which is an SVM saved to disk as\na Python pickle.\n\n\nTraining uses \nscikit-learn\n to perform\na grid search over SVM parameters.\nFor 1000's of images, training the SVMs takes seconds.\nOur trained model obtains 87% accuracy on this set of data.\n\n\nClassifying New Images\n\n\nWe have released a \nceleb-classifier.nn4.v1.pkl\n classification model\nthat is trained on about 6000 total images of the following people,\nwhich are the people with the most images in our dataset.\nClassifiers can be created with far less images per\nperson.\n\n\n\n\nAmerica Ferrera\n\n\nAmy Adams\n\n\nAnne Hathaway\n\n\nBen Stiller\n\n\nBradley Cooper\n\n\nDavid Boreanaz\n\n\nEmily Deschanel\n\n\nEva Longoria\n\n\nJon Hamm\n\n\nSteve Carell\n\n\n\n\nFor an example, consider the following small set of images\nthe model has no knowledge of.\nFor an unknown person, a prediction still needs to be made, but\nthe confidence score is usually lower.\n\n\nRun the classifier on your images with:\n\n\n./demos/classifier.py infer ./models/openface/celeb-classifier.nn4.v1.pkl ./your-image.png\n\n\n\n\n\n\n\n\n\n\nPerson\n\n\nImage\n\n\nPrediction\n\n\nConfidence\n\n\n\n\n\n\n\n\n\n\nCarell\n\n\n\n\nSteveCarell\n\n\n0.96\n\n\n\n\n\n\nAdams\n\n\n\n\nAmyAdams\n\n\n0.98\n\n\n\n\n\n\nLennon 1 (Unknown)\n\n\n\n\nDavidBoreanaz\n\n\n0.27\n\n\n\n\n\n\nLennon 2 (Unknown)\n\n\n\n\nDavidBoreanaz\n\n\n0.43\n\n\n\n\n\n\n\n\nMinimal Working Example to Extract Features\n\n\nmkdir -p classify-test/raw/{lennon,clapton}\ncp images/examples/lennon-* classify-test/raw/lennon\ncp images/examples/clapton-* classify-test/raw/clapton\n./util/align-dlib.py classify-test/raw align innerEyesAndBottomLip classify-test/aligned --size 96\n./batch-represent/main.lua -outDir classify-test/features -data classify-test/aligned\n...\nnImgs: \u00a04\nRepresent: 4/4",
            "title": "Demo 3 - Training a Classifier"
        },
        {
            "location": "/demo-3-classifier/#demo-3-training-a-classifier",
            "text": "OpenFace's core provides a feature extraction method to\nobtain a low-dimensional representation of any face. demos/classifier.py \nshows a demo of how these representations can be\nused to create a face classifier.  There is a distinction between training the deep neural network (DNN)\nmodel for feature representation\nand training a model for classifying people with the DNN model.\nThis shows how to use a pre-trained DNN model to train and use\na classification model.",
            "title": "Demo 3: Training a Classifier"
        },
        {
            "location": "/demo-3-classifier/#creating-a-classification-model",
            "text": "1. Create raw image directory.  Create a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as  jpg  or  png  and have\na lowercase extension.  $ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png  2. Preprocess the raw images  Change  8  to however many\nseparate processes you want to run: for N in {1..8}; do ./util/align-dlib.py  path-to-raw-data  align innerEyesAndBottomLip  path-to-aligned-data  --size 96   done .  3. Generate Representations  ./batch-represent/main.lua -outDir  feature-directory  -data  path-to-aligned-data \ncreates  reps.csv  and  labels.csv  in  feature-directory .  4. Create the Classification Model  Use  ./demos/classifier.py train  feature-directory  to produce\nthe classification model which is an SVM saved to disk as\na Python pickle.  Training uses  scikit-learn  to perform\na grid search over SVM parameters.\nFor 1000's of images, training the SVMs takes seconds.\nOur trained model obtains 87% accuracy on this set of data.",
            "title": "Creating a Classification Model"
        },
        {
            "location": "/demo-3-classifier/#classifying-new-images",
            "text": "We have released a  celeb-classifier.nn4.v1.pkl  classification model\nthat is trained on about 6000 total images of the following people,\nwhich are the people with the most images in our dataset.\nClassifiers can be created with far less images per\nperson.   America Ferrera  Amy Adams  Anne Hathaway  Ben Stiller  Bradley Cooper  David Boreanaz  Emily Deschanel  Eva Longoria  Jon Hamm  Steve Carell   For an example, consider the following small set of images\nthe model has no knowledge of.\nFor an unknown person, a prediction still needs to be made, but\nthe confidence score is usually lower.  Run the classifier on your images with:  ./demos/classifier.py infer ./models/openface/celeb-classifier.nn4.v1.pkl ./your-image.png     Person  Image  Prediction  Confidence      Carell   SteveCarell  0.96    Adams   AmyAdams  0.98    Lennon 1 (Unknown)   DavidBoreanaz  0.27    Lennon 2 (Unknown)   DavidBoreanaz  0.43",
            "title": "Classifying New Images"
        },
        {
            "location": "/demo-3-classifier/#minimal-working-example-to-extract-features",
            "text": "mkdir -p classify-test/raw/{lennon,clapton}\ncp images/examples/lennon-* classify-test/raw/lennon\ncp images/examples/clapton-* classify-test/raw/clapton\n./util/align-dlib.py classify-test/raw align innerEyesAndBottomLip classify-test/aligned --size 96\n./batch-represent/main.lua -outDir classify-test/features -data classify-test/aligned\n...\nnImgs: \u00a04\nRepresent: 4/4",
            "title": "Minimal Working Example to Extract Features"
        },
        {
            "location": "/usage/",
            "text": "Usage\n\n\nExisting Models\n\n\nSee \nthe image comparison demo\n for a complete example\nwritten in Python using a naive Torch subprocess to process the faces.\n\n\nimport openface\nfrom openface.alignment import NaiveDlib # Depends on dlib.\n\n# `args` are parsed command-line arguments.\n\nalign = NaiveDlib(args.dlibFaceMean, args.dlibFacePredictor)\nnet = openface.TorchWrap(args.networkModel, imgDim=args.imgDim, cuda=args.cuda)\n\n# `img` is a numpy matrix containing the RGB pixels of the image.\nbb = align.getLargestFaceBoundingBox(img)\nalignedFace = align.alignImg(\naffine\n, args.imgDim, img, bb)\nrep1 = net.forwardImage(alignedFace)\n\n# `rep2` obtained similarly.\nd = rep1 - rep2\ndistance = np.dot(d, d)",
            "title": "Usage"
        },
        {
            "location": "/usage/#usage",
            "text": "",
            "title": "Usage"
        },
        {
            "location": "/usage/#existing-models",
            "text": "See  the image comparison demo  for a complete example\nwritten in Python using a naive Torch subprocess to process the faces.  import openface\nfrom openface.alignment import NaiveDlib # Depends on dlib.\n\n# `args` are parsed command-line arguments.\n\nalign = NaiveDlib(args.dlibFaceMean, args.dlibFacePredictor)\nnet = openface.TorchWrap(args.networkModel, imgDim=args.imgDim, cuda=args.cuda)\n\n# `img` is a numpy matrix containing the RGB pixels of the image.\nbb = align.getLargestFaceBoundingBox(img)\nalignedFace = align.alignImg( affine , args.imgDim, img, bb)\nrep1 = net.forwardImage(alignedFace)\n\n# `rep2` obtained similarly.\nd = rep1 - rep2\ndistance = np.dot(d, d)",
            "title": "Existing Models"
        },
        {
            "location": "/setup/",
            "text": "Setup\n\n\nThe following instructions are for Linux and OSX only.\nPlease contribute modifications and build instructions if you\nare interested in running this on other operating systems.\n\n\n\n\nWe strongly recommend using the \nDocker\n\n  container unless you are experienced with building\n  Linux software from source.\n\n\nIn OSX, you may have to change the hashbangs\n  from \npython2\n to \npython\n.\n\n\nOpenFace has been tested in Ubuntu 14.04 and OSX 10.10\n  and may not work well on other distributions.\n  Please let us know of any challenges you had to overcome\n  getting OpenFace to work on other distributions.\n\n\n\n\nWarning for architectures other than 64-bit x86\n\n\nSee \n#42\n.\n\n\nCheck out git submodules\n\n\nClone with \n--recursive\n or run \ngit submodule init \n git submodule update\n\nafter checking out.\n\n\nDownload the models\n\n\nRun \nmodels/get-models.sh\n\nto download pre-trained OpenFace\nmodels on the combined CASIA-WebFace and FaceScrub database.\nThis also downloads dlib's pre-trained model for face landmark detection.\nThis will incur about 500MB of network traffic for the compressed\nmodels that will decompress to about 1GB on disk.\n\n\nBe sure the md5 checksums match the following.\nUse \nmd5sum\n in Linux and \nmd5\n in OSX.\n\n\nopenface(master)$ md5sum models/{dlib/*.dat,openface/*.{pkl,t7}}\n73fde5e05226548677a050913eed4e04  models/dlib/shape_predictor_68_face_landmarks.dat\nc0675d57dc976df601b085f4af67ecb9  models/openface/celeb-classifier.nn4.v1.pkl\na59a5ec1938370cd401b257619848960  models/openface/nn4.v1.t7\n\n\n\n\nWith Docker\n\n\nThis repo can be deployed as a container with \nDocker\n\nfor CPU mode.\nBe sure you have checked out the submodules and downloaded\nthe models as described above.\nDepending on your Docker configuration, you may need to\nrun the docker commands as root.\n\n\nTo use, place your images in \nopenface\n on your host and\naccess them from the shared Docker directory.\n\n\ndocker build -t openface ./docker\ndocker run -p 9000:9000 -t -i -v $PWD:/openface openface /bin/bash\ncd /openface\n./demos/compare.py images/examples/{lennon*,clapton*}\n\n\n\n\nDocker in OSX\n\n\nIn OSX, follow the\n\nDocker Mac OSX Installation Guide\n\nand start a docker machine and connect your shell to it\nbefore trying to build the container.\nIn the simplest case, this can be done with:\n\n\ndocker-machine create --driver virtualbox --virtualbox-memory 4096 default\neval $(docker-machine env default)\n\n\n\n\nDocker memory issues in OSX\n\n\nSome users have reported the following silent Torch/Lua failure\nwhen running \nbatch-represent\n caused by an out of memory issue.\n\n\n/root/torch/install/bin/luajit: /openface/batch-represent/dataset.lua:191: attempt to perform arithmetic on a nil value\n\n\n\n\nIf you're experiencing this, make sure you have created a Docker machine\nwith at least 4GB of memory with \n--virtualbox-memory 4096\n.\n\n\nBy hand\n\n\nBe sure you have checked out the submodules and downloaded the models as\ndescribed above.\nSee the\n\nDockerfile\n\nas a reference.\n\n\nThis project uses \npython2\n because of the \nopencv\n\nand \ndlib\n dependencies.\nInstall the packages the Dockerfile uses with your package manager.\nWith \npip2\n, install \nnumpy\n, \npandas\n, \nscipy\n, \nscikit-learn\n, and \nscikit-image\n.\n\n\nNext, manually install the following.\n\n\nOpenCV\n\n\nDownload \nOpenCV 2.4.11\n\nand follow their\n\nbuild instructions\n.\n\n\ndlib\n\n\ndlib can alternatively by installed from \npypi\n,\nbut might be slower than building manually because they are not\ncompiled with AVX support.\n\n\ndlib requires boost libraries to be installed.\n\n\nTo build manually, start by\ndownloading\n\ndlib v18.16\n,\nthen:\n\n\nmkdir -p ~/src\ncd ~/src\ntar xf dlib-18.16.tar.bz2\ncd dlib-18.16/python_examples\nmkdir build\ncd build\ncmake ../../tools/python\ncmake --build . --config Release\ncp dlib.so ..\n\n\n\n\nAt this point, you should be able to start your \npython2\n\ninterpreter and successfully run \nimport cv2; import dlib\n.\n\n\nIn OSX, you may get a \nFatal Python error: PyThreadState_Get: no current thread\n.\nYou may be able to resolve by rebuilding \npython\n and \nboost-python\n\nas reported in \n#21\n,\nbut please file a new issue with us or \ndlib\n\nif you are unable to resolve this.\n\n\nTorch\n\n\nInstall \nTorch\n from the instructions on their website\nand install the dependencies with \nluarocks install $NAME\n,\nwhere \n$NAME\n is as listed below.\n\n\n\n\ndpnn\n\n\nnn\n\n\noptim\n\n\ncsvigo\n\n\ncudnn.torch\n (only for CUDA support)\n\n\nfblualib\n\n  (only for \ntraining a DNN\n)\n\n\n\n\nAt this point, the command-line program \nth\n should\nbe available in your shell.",
            "title": "Setup"
        },
        {
            "location": "/setup/#setup",
            "text": "The following instructions are for Linux and OSX only.\nPlease contribute modifications and build instructions if you\nare interested in running this on other operating systems.   We strongly recommend using the  Docker \n  container unless you are experienced with building\n  Linux software from source.  In OSX, you may have to change the hashbangs\n  from  python2  to  python .  OpenFace has been tested in Ubuntu 14.04 and OSX 10.10\n  and may not work well on other distributions.\n  Please let us know of any challenges you had to overcome\n  getting OpenFace to work on other distributions.",
            "title": "Setup"
        },
        {
            "location": "/setup/#warning-for-architectures-other-than-64-bit-x86",
            "text": "See  #42 .",
            "title": "Warning for architectures other than 64-bit x86"
        },
        {
            "location": "/setup/#check-out-git-submodules",
            "text": "Clone with  --recursive  or run  git submodule init   git submodule update \nafter checking out.",
            "title": "Check out git submodules"
        },
        {
            "location": "/setup/#download-the-models",
            "text": "Run  models/get-models.sh \nto download pre-trained OpenFace\nmodels on the combined CASIA-WebFace and FaceScrub database.\nThis also downloads dlib's pre-trained model for face landmark detection.\nThis will incur about 500MB of network traffic for the compressed\nmodels that will decompress to about 1GB on disk.  Be sure the md5 checksums match the following.\nUse  md5sum  in Linux and  md5  in OSX.  openface(master)$ md5sum models/{dlib/*.dat,openface/*.{pkl,t7}}\n73fde5e05226548677a050913eed4e04  models/dlib/shape_predictor_68_face_landmarks.dat\nc0675d57dc976df601b085f4af67ecb9  models/openface/celeb-classifier.nn4.v1.pkl\na59a5ec1938370cd401b257619848960  models/openface/nn4.v1.t7",
            "title": "Download the models"
        },
        {
            "location": "/setup/#with-docker",
            "text": "This repo can be deployed as a container with  Docker \nfor CPU mode.\nBe sure you have checked out the submodules and downloaded\nthe models as described above.\nDepending on your Docker configuration, you may need to\nrun the docker commands as root.  To use, place your images in  openface  on your host and\naccess them from the shared Docker directory.  docker build -t openface ./docker\ndocker run -p 9000:9000 -t -i -v $PWD:/openface openface /bin/bash\ncd /openface\n./demos/compare.py images/examples/{lennon*,clapton*}  Docker in OSX  In OSX, follow the Docker Mac OSX Installation Guide \nand start a docker machine and connect your shell to it\nbefore trying to build the container.\nIn the simplest case, this can be done with:  docker-machine create --driver virtualbox --virtualbox-memory 4096 default\neval $(docker-machine env default)  Docker memory issues in OSX  Some users have reported the following silent Torch/Lua failure\nwhen running  batch-represent  caused by an out of memory issue.  /root/torch/install/bin/luajit: /openface/batch-represent/dataset.lua:191: attempt to perform arithmetic on a nil value  If you're experiencing this, make sure you have created a Docker machine\nwith at least 4GB of memory with  --virtualbox-memory 4096 .",
            "title": "With Docker"
        },
        {
            "location": "/setup/#by-hand",
            "text": "Be sure you have checked out the submodules and downloaded the models as\ndescribed above.\nSee the Dockerfile \nas a reference.  This project uses  python2  because of the  opencv \nand  dlib  dependencies.\nInstall the packages the Dockerfile uses with your package manager.\nWith  pip2 , install  numpy ,  pandas ,  scipy ,  scikit-learn , and  scikit-image .  Next, manually install the following.  OpenCV  Download  OpenCV 2.4.11 \nand follow their build instructions .  dlib  dlib can alternatively by installed from  pypi ,\nbut might be slower than building manually because they are not\ncompiled with AVX support.  dlib requires boost libraries to be installed.  To build manually, start by\ndownloading dlib v18.16 ,\nthen:  mkdir -p ~/src\ncd ~/src\ntar xf dlib-18.16.tar.bz2\ncd dlib-18.16/python_examples\nmkdir build\ncd build\ncmake ../../tools/python\ncmake --build . --config Release\ncp dlib.so ..  At this point, you should be able to start your  python2 \ninterpreter and successfully run  import cv2; import dlib .  In OSX, you may get a  Fatal Python error: PyThreadState_Get: no current thread .\nYou may be able to resolve by rebuilding  python  and  boost-python \nas reported in  #21 ,\nbut please file a new issue with us or  dlib \nif you are unable to resolve this.  Torch  Install  Torch  from the instructions on their website\nand install the dependencies with  luarocks install $NAME ,\nwhere  $NAME  is as listed below.   dpnn  nn  optim  csvigo  cudnn.torch  (only for CUDA support)  fblualib \n  (only for  training a DNN )   At this point, the command-line program  th  should\nbe available in your shell.",
            "title": "By hand"
        },
        {
            "location": "/faq/",
            "text": "FAQ\n\n\nHow can I make OpenFace run faster?\n\n\n\n\n\n\nResize your images so that faces are approximately 100x100 pixels\n  before running detection and alignment.\n\n\n\n\n\n\nCompile dlib with AVX instructions, as discussed\n  \nhere\n.\n  Use the \n-DUSE_AVX_INSTRUCTIONS=ON\n in the first \ncmake\n command.\n  If your architecture does not support AVX, try SSE4 or SSE2.",
            "title": "FAQ"
        },
        {
            "location": "/faq/#faq",
            "text": "",
            "title": "FAQ"
        },
        {
            "location": "/faq/#how-can-i-make-openface-run-faster",
            "text": "Resize your images so that faces are approximately 100x100 pixels\n  before running detection and alignment.    Compile dlib with AVX instructions, as discussed\n   here .\n  Use the  -DUSE_AVX_INSTRUCTIONS=ON  in the first  cmake  command.\n  If your architecture does not support AVX, try SSE4 or SSE2.",
            "title": "How can I make OpenFace run faster?"
        },
        {
            "location": "/accuracy/",
            "text": "Cool demos, but I want numbers. What's the accuracy?\n\n\nEven though the public datasets we trained on have orders of magnitude less data\nthan private industry datasets, the accuracy is remarkably high\non the standard\n\nLFW\n\nbenchmark.\nWe had to fallback to using the deep funneled versions for\n58 of 13233 images because dlib failed to detect a face or landmarks.\nWe obtain a mean accuracy of 0.8138 \n 0.0149 with an AUC of 0.893.\nFor comparison, training with Google-scale data results in an\naccuracy of .9963 \n 0.009.\n\n\n\n\nThis can be generated with the following commands from the root \nopenface\n\ndirectory, assuming you have downloaded and placed the raw and\n\ndeep funneled\n\nLFW data from \nhere\n\nin \n./data/lfw/raw\n and \n./data/lfw/deepfunneled\n.\n\n\n\n\nInstall prerequisites as below.\n\n\nPreprocess the raw \nlfw\n images, change \n8\n to however many\n   separate processes you want to run:\n   \nfor N in {1..8}; do ./util/align-dlib.py data/lfw/raw align innerEyesAndBottomLip data/lfw/dlib-affine-sz:96 --size 96 \n done\n.\n   Fallback to deep funneled versions for images that dlib failed\n   to align:\n   \n./util/align-dlib.py data/lfw/raw align innerEyesAndBottomLip data/lfw/dlib-affine-sz:96 --size 96 --fallbackLfw data/lfw/deepfunneled\n\n\nGenerate representations with \n./batch-represent/main.lua -outDir evaluation/lfw.nn4.v1.reps -model models/openface/nn4.v1.t7 -data data/lfw/dlib-affine-sz:96\n\n\nGenerate the ROC curve from the \nevaluation\n directory with \n./lfw-roc.py --workDir lfw.nn4.v1.reps\n.\n   This creates \nroc.pdf\n in the \nlfw.nn4.v1.reps\n directory.\n\n\n\n\n\n\nIf you're interested in higher accuracy open source code, see:\n\n\nOxford's VGG Face Descriptor\n\n\nThis is licensed for non-commercial research purposes.\nThey've released their softmax network, which obtains .9727 accuracy\non the LFW and will release their triplet network (0.9913 accuracy)\nand data soon (?).\n\n\nTheir softmax model doesn't embed features like FaceNet,\nwhich makes tasks like classification and clustering more difficult.\nTheir triplet model hasn't yet been released, but will provide\nembeddings similar to FaceNet.\nThe triplet model will be supported by OpenFace once it's released.\n\n\nAlfredXiangWu/face_verification_experiment\n\n\nThis uses Caffe and doesn't yet have a license.\nThe accuracy on the LFW is .9777.\nThis model doesn't embed features like FaceNet,\nwhich makes tasks like classification and clustering more difficult.",
            "title": "Accuracy"
        },
        {
            "location": "/accuracy/#cool-demos-but-i-want-numbers-whats-the-accuracy",
            "text": "Even though the public datasets we trained on have orders of magnitude less data\nthan private industry datasets, the accuracy is remarkably high\non the standard LFW \nbenchmark.\nWe had to fallback to using the deep funneled versions for\n58 of 13233 images because dlib failed to detect a face or landmarks.\nWe obtain a mean accuracy of 0.8138   0.0149 with an AUC of 0.893.\nFor comparison, training with Google-scale data results in an\naccuracy of .9963   0.009.   This can be generated with the following commands from the root  openface \ndirectory, assuming you have downloaded and placed the raw and deep funneled \nLFW data from  here \nin  ./data/lfw/raw  and  ./data/lfw/deepfunneled .   Install prerequisites as below.  Preprocess the raw  lfw  images, change  8  to however many\n   separate processes you want to run:\n    for N in {1..8}; do ./util/align-dlib.py data/lfw/raw align innerEyesAndBottomLip data/lfw/dlib-affine-sz:96 --size 96   done .\n   Fallback to deep funneled versions for images that dlib failed\n   to align:\n    ./util/align-dlib.py data/lfw/raw align innerEyesAndBottomLip data/lfw/dlib-affine-sz:96 --size 96 --fallbackLfw data/lfw/deepfunneled  Generate representations with  ./batch-represent/main.lua -outDir evaluation/lfw.nn4.v1.reps -model models/openface/nn4.v1.t7 -data data/lfw/dlib-affine-sz:96  Generate the ROC curve from the  evaluation  directory with  ./lfw-roc.py --workDir lfw.nn4.v1.reps .\n   This creates  roc.pdf  in the  lfw.nn4.v1.reps  directory.    If you're interested in higher accuracy open source code, see:",
            "title": "Cool demos, but I want numbers. What's the accuracy?"
        },
        {
            "location": "/accuracy/#oxfords-vgg-face-descriptor",
            "text": "This is licensed for non-commercial research purposes.\nThey've released their softmax network, which obtains .9727 accuracy\non the LFW and will release their triplet network (0.9913 accuracy)\nand data soon (?).  Their softmax model doesn't embed features like FaceNet,\nwhich makes tasks like classification and clustering more difficult.\nTheir triplet model hasn't yet been released, but will provide\nembeddings similar to FaceNet.\nThe triplet model will be supported by OpenFace once it's released.",
            "title": "Oxford's VGG Face Descriptor"
        },
        {
            "location": "/accuracy/#alfredxiangwuface_verification_experiment",
            "text": "This uses Caffe and doesn't yet have a license.\nThe accuracy on the LFW is .9777.\nThis model doesn't embed features like FaceNet,\nwhich makes tasks like classification and clustering more difficult.",
            "title": "AlfredXiangWu/face_verification_experiment"
        },
        {
            "location": "/models/",
            "text": "Model Definitions\n\n\nModel definitions should be kept in \nmodels/openface\n,\nwhere we have provided definitions of the \nNN2\n\nand \nnn4\n as described in the paper,\nbut with batch normalization and no normalization in the lower layers.\nThe inception layers are introduced  in\n\nGoing Deeper with Convolutions\n\nby Christian Szegedy et al.\n\n\nPre-trained Models\n\n\nPre-trained models are versioned and should be released with\na corresponding model definition.\nWe currently only provide a pre-trained model for \nnn4.v1\n\nbecause we have limited access to large-scale face recognition\ndatasets.\n\n\nnn4.v1\n\n\nThis model has been trained by combining the two largest (of August 2015)\npublicly-available face recognition datasets based on names:\n\nFaceScrub\n\nand \nCASIA-WebFace\n.\nThis model was trained for about 300 hours on a Tesla K40 GPU.\n\n\nThe following plot shows the triplet loss on the training\nand test set.\nEach training epoch is defined to be 1000 minibatches, where\neach minibatch processes 100 triplets.\nEach testing epoch is defined to be 300 minibatches,\nwhere each minibatch processes 100 triplets.\nSemi-hard triplets are used on the training set, and\nrandom triplets are used on the testing set.\nOur \nnn4.v1\n model is from epoch 177.\n\n\nThe LFW section above shows that this model obtains a mean\naccuracy of 0.8483 \n 0.0172 with an AUC of 0.923.",
            "title": "Pre-Trained Models"
        },
        {
            "location": "/models/#model-definitions",
            "text": "Model definitions should be kept in  models/openface ,\nwhere we have provided definitions of the  NN2 \nand  nn4  as described in the paper,\nbut with batch normalization and no normalization in the lower layers.\nThe inception layers are introduced  in Going Deeper with Convolutions \nby Christian Szegedy et al.",
            "title": "Model Definitions"
        },
        {
            "location": "/models/#pre-trained-models",
            "text": "Pre-trained models are versioned and should be released with\na corresponding model definition.\nWe currently only provide a pre-trained model for  nn4.v1 \nbecause we have limited access to large-scale face recognition\ndatasets.",
            "title": "Pre-trained Models"
        },
        {
            "location": "/models/#nn4v1",
            "text": "This model has been trained by combining the two largest (of August 2015)\npublicly-available face recognition datasets based on names: FaceScrub \nand  CASIA-WebFace .\nThis model was trained for about 300 hours on a Tesla K40 GPU.  The following plot shows the triplet loss on the training\nand test set.\nEach training epoch is defined to be 1000 minibatches, where\neach minibatch processes 100 triplets.\nEach testing epoch is defined to be 300 minibatches,\nwhere each minibatch processes 100 triplets.\nSemi-hard triplets are used on the training set, and\nrandom triplets are used on the testing set.\nOur  nn4.v1  model is from epoch 177.  The LFW section above shows that this model obtains a mean\naccuracy of 0.8483   0.0172 with an AUC of 0.923.",
            "title": "nn4.v1"
        },
        {
            "location": "/training-new-models/",
            "text": "Training new neural network models\n\n\nWe have also released our deep neural network (DNN)\ntraining infrastructure to promote an open ecosystem and enable quicker\nbootstrapping for new research and development.\n\n\nThere is a distinction between training the DNN model for feature representation\nand training a model for classifying people with the DNN model.\nIf you're interested in creating a new classifier,\nsee \nDemo 3\n.\nThis page is for advanced users interested in training a new DNN model\nand should be done with large datasets (\n500k images) to improve the\nfeature representation.\n\n\nWarning:\n Training is computationally and memory expensive and takes a\nfew weeks on our Tesla K40 GPU.\nBecause of this, the training code assumes CUDA is installed.\n\n\nA rough overview of training is:\n\n\n1. Create raw image directory.\n\n\nCreate a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as \njpg\n or \npng\n and have\na lowercase extension.\n\n\n$ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png\n\n\n\n\n2. Preprocess the raw images\n\n\nChange \n8\n to however many\nseparate processes you want to run:\n\nfor N in {1..8}; do ./util/align-dlib.py \npath-to-raw-data\n align innerEyesAndBottomLip \npath-to-aligned-data\n --size 96 \n done\n.\nPrune out directories with less than N (I use 10) images\nper class with \n./util/prune-dataset.py \npath-to-aligned-data\n --numImagesThreshold \nN\n and\nthen split the dataset into \ntrain\n and \nval\n subdirectories\nwith \n./util/create-train-val-split.py \npath-to-aligned-data\n \nvalidation-ratio\n.\n\n\n3. Train the model\n\n\nRun \ntraining/main.lua\n to start training the model.\nEdit the dataset options in \ntraining/opts.lua\n or\npass them as command-line parameters.\nThis will output the loss and in-progress models to \ntraining/work\n.\nThe default minibatch size (parameter \n-batchSize\n) is 100 and requires\nabout 10GB of GPU memory.\n\n\nWarning: Metadata about the on-disk data is cached in\n\ntraining/work/{train,test}Cache.t7\n and assumes\nthe data directory does not change.\nIf your data directory changes, delete these\nfiles so they will be regenerated.\n\n\nStopping and starting training\n\n\nModels are saved in the \nwork\n directory after every epoch.\nIf the training process is killed, it can be resumed from\nthe last saved model with the \n-retrain\n option.\nAlso pass a different \n-manualSeed\n so a different image\nsequence is sampled and correctly set \n-epochNumber\n.\n\n\n4. Analyze training\n\n\nVisualize the loss with \ntraining/plot-loss.py\n.",
            "title": "Training a DNN Model"
        },
        {
            "location": "/training-new-models/#training-new-neural-network-models",
            "text": "We have also released our deep neural network (DNN)\ntraining infrastructure to promote an open ecosystem and enable quicker\nbootstrapping for new research and development.  There is a distinction between training the DNN model for feature representation\nand training a model for classifying people with the DNN model.\nIf you're interested in creating a new classifier,\nsee  Demo 3 .\nThis page is for advanced users interested in training a new DNN model\nand should be done with large datasets ( 500k images) to improve the\nfeature representation.  Warning:  Training is computationally and memory expensive and takes a\nfew weeks on our Tesla K40 GPU.\nBecause of this, the training code assumes CUDA is installed.  A rough overview of training is:",
            "title": "Training new neural network models"
        },
        {
            "location": "/training-new-models/#1-create-raw-image-directory",
            "text": "Create a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as  jpg  or  png  and have\na lowercase extension.  $ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png",
            "title": "1. Create raw image directory."
        },
        {
            "location": "/training-new-models/#2-preprocess-the-raw-images",
            "text": "Change  8  to however many\nseparate processes you want to run: for N in {1..8}; do ./util/align-dlib.py  path-to-raw-data  align innerEyesAndBottomLip  path-to-aligned-data  --size 96   done .\nPrune out directories with less than N (I use 10) images\nper class with  ./util/prune-dataset.py  path-to-aligned-data  --numImagesThreshold  N  and\nthen split the dataset into  train  and  val  subdirectories\nwith  ./util/create-train-val-split.py  path-to-aligned-data   validation-ratio .",
            "title": "2. Preprocess the raw images"
        },
        {
            "location": "/training-new-models/#3-train-the-model",
            "text": "Run  training/main.lua  to start training the model.\nEdit the dataset options in  training/opts.lua  or\npass them as command-line parameters.\nThis will output the loss and in-progress models to  training/work .\nThe default minibatch size (parameter  -batchSize ) is 100 and requires\nabout 10GB of GPU memory.  Warning: Metadata about the on-disk data is cached in training/work/{train,test}Cache.t7  and assumes\nthe data directory does not change.\nIf your data directory changes, delete these\nfiles so they will be regenerated.  Stopping and starting training  Models are saved in the  work  directory after every epoch.\nIf the training process is killed, it can be resumed from\nthe last saved model with the  -retrain  option.\nAlso pass a different  -manualSeed  so a different image\nsequence is sampled and correctly set  -epochNumber .",
            "title": "3. Train the model"
        },
        {
            "location": "/training-new-models/#4-analyze-training",
            "text": "Visualize the loss with  training/plot-loss.py .",
            "title": "4. Analyze training"
        },
        {
            "location": "/visualizations/",
            "text": "Visualizing representations with t-SNE\n\n\nt-SNE\n is a dimensionality\nreduction technique that can be used to visualize the\n128-dimensional features OpenFace produces.\nThe following shows the visualization of the three people\nin the training and testing dataset with the most images.\n\n\nTraining\n\n\n\n\nTesting\n\n\n\n\nThese can be generated with the following commands from the root\n\nopenface\n directory.\n\n\n1. Create raw image directory.\n\n\nCreate a directory for a subset of raw images that you want to visualize\nwith TSNE.\nMake images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as \njpg\n or \npng\n and have\na lowercase extension.\n\n\n$ tree data/mydataset-subset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png\n\n\n\n\n2. Preprocess the raw images\n\n\nChange \n8\n to however many\nseparate processes you want to run:\n\nfor N in {1..8}; do ./util/align-dlib.py \npath-to-raw-data\n align innerEyesAndBottomLip \npath-to-aligned-data\n --size 96 \n done\n.\n\n\n3. Generate Representations\n\n\n./batch-represent/main.lua -outDir \nfeature-directory\n -data \npath-to-aligned-data\n\ncreates \nreps.csv\n and \nlabels.csv\n in \nfeature-directory\n.\n\n\n4. Generate TSNE visualization\n\n\nGenerate the t-SNE visualization with\n\n./util/tsne.py \nfeature-directory\n --names \nname 1\n ... \nname n\n,\nwhere \nname i\n corresponds to label \ni\n from the\nleft-most column in \nlabels.csv\n.\nThis corr\nThis creates \ntsne.pdf\n in \nfeature-directory\n.\n\n\nVisualizing layer outputs\n\n\nVisualizing the output feature maps of each layer\nis sometimes helpful to understand what features\nthe network has learned to extract.\nWith faces, the locations of the eyes, nose, and\nmouth should play an important role.\n\n\ndemos/vis-outputs.lua\n\noutputs the feature maps from an aligned image.\nThe following shows the first 39 filters of the\nfirst convolutional layer on two images\nof John Lennon.",
            "title": "Visualizations"
        },
        {
            "location": "/visualizations/#visualizing-representations-with-t-sne",
            "text": "t-SNE  is a dimensionality\nreduction technique that can be used to visualize the\n128-dimensional features OpenFace produces.\nThe following shows the visualization of the three people\nin the training and testing dataset with the most images.  Training   Testing   These can be generated with the following commands from the root openface  directory.",
            "title": "Visualizing representations with t-SNE"
        },
        {
            "location": "/visualizations/#1-create-raw-image-directory",
            "text": "Create a directory for a subset of raw images that you want to visualize\nwith TSNE.\nMake images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as  jpg  or  png  and have\na lowercase extension.  $ tree data/mydataset-subset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png",
            "title": "1. Create raw image directory."
        },
        {
            "location": "/visualizations/#2-preprocess-the-raw-images",
            "text": "Change  8  to however many\nseparate processes you want to run: for N in {1..8}; do ./util/align-dlib.py  path-to-raw-data  align innerEyesAndBottomLip  path-to-aligned-data  --size 96   done .",
            "title": "2. Preprocess the raw images"
        },
        {
            "location": "/visualizations/#3-generate-representations",
            "text": "./batch-represent/main.lua -outDir  feature-directory  -data  path-to-aligned-data \ncreates  reps.csv  and  labels.csv  in  feature-directory .",
            "title": "3. Generate Representations"
        },
        {
            "location": "/visualizations/#4-generate-tsne-visualization",
            "text": "Generate the t-SNE visualization with ./util/tsne.py  feature-directory  --names  name 1  ...  name n ,\nwhere  name i  corresponds to label  i  from the\nleft-most column in  labels.csv .\nThis corr\nThis creates  tsne.pdf  in  feature-directory .",
            "title": "4. Generate TSNE visualization"
        },
        {
            "location": "/visualizations/#visualizing-layer-outputs",
            "text": "Visualizing the output feature maps of each layer\nis sometimes helpful to understand what features\nthe network has learned to extract.\nWith faces, the locations of the eyes, nose, and\nmouth should play an important role.  demos/vis-outputs.lua \noutputs the feature maps from an aligned image.\nThe following shows the first 39 filters of the\nfirst convolutional layer on two images\nof John Lennon.",
            "title": "Visualizing layer outputs"
        },
        {
            "location": "/release-notes/",
            "text": "Release Notes\n\n\n0.1.1 (2015/10/15)\n\n\n\n\nFix debug mode of NaiveDlib alignment.\n\n\nAdd util/prune-dataset.py for dataset processing.\n\n\nCorrect Docker dependencies.\n\n\n\n\n0.1.0 (2015/10/13)\n\n\n\n\nInitial release.",
            "title": "Release Notes"
        },
        {
            "location": "/release-notes/#release-notes",
            "text": "",
            "title": "Release Notes"
        },
        {
            "location": "/release-notes/#011-20151015",
            "text": "Fix debug mode of NaiveDlib alignment.  Add util/prune-dataset.py for dataset processing.  Correct Docker dependencies.",
            "title": "0.1.1 (2015/10/15)"
        },
        {
            "location": "/release-notes/#010-20151013",
            "text": "Initial release.",
            "title": "0.1.0 (2015/10/13)"
        }
    ]
}