{
    "docs": [
        {
            "location": "/",
            "text": "OpenFace \n\n\n\n\nFree and open source face recognition with\nGoogle's FaceNet deep neural network.\n\n\n\n\n\n\nOpenFace is a Python and \nTorch\n implementation of the CVPR\n2015 paper\n\nFaceNet: A Unified Embedding for Face Recognition and Clustering\n\nby Florian Schroff, Dmitry Kalenichenko, and James Philbin at Google.\nTorch allows the network to be executed on a CPU or with CUDA.\n\n\nCrafted by \nBrandon Amos\n in the\n\nElijah\n research group at\nCarnegie Mellon University.\n\n\n\n\n\n\nThe code is available on GitHub at\n  \ncmusatyalab/openface\n.\n\n\nJoin the\n  \ncmu-openface group\n\n  or the\n  \ngitter chat\n\n  for discussions and installation issues.\n\n\nDevelopment discussions and bugs reports are on the\n  \nissue tracker\n.\n\n\n\n\n\n\nIsn't face recognition a solved problem?\n\n\nNo! Accuracies from research papers have just begun to surpass\nhuman accuracies on some benchmarks.\nThe accuracies of open source face recognition systems lag\nbehind the state-of-the-art.\nSee our accuracy comparisons on the famous LFW benchmark below.\n\n\n\n\nPlease use responsibly!\n\n\nWe do not support the use of this project in applications\nthat violate privacy and security.\nWe are using this to help cognitively impaired users to\nsense and understand the world around them.\n\n\n\n\nOverview\n\n\nThe following overview shows the workflow for a single input\nimage of Sylvestor Stallone from the publicly available\n\nLFW dataset\n.\n\n\n\n\nDetect faces with a pre-trained models from\n  \ndlib\n\n  or\n  \nOpenCV\n.\n\n\nTransform the face for the neural network.\n   This repository uses dlib's\n   \nreal-time pose estimation\n\n   with OpenCV's\n   \naffine transformation\n\n   to try to make the eyes and bottom lip appear in\n   the same location on each image.\n\n\nUse a deep neural network to represent (or embed) the face on\n   a 128-dimensional unit hypersphere.\n   The embedding is a generic representation for anybody's face.\n   Unlike other face representations, this embedding has the nice property\n   that a larger distance between two face embeddings means\n   that the faces are likely not of the same person.\n   This property makes clustering, similarity detection,\n   and classification tasks easier than other face recognition\n   techniques where the Euclidean distance between\n   features is not meaningful.\n\n\nApply your favorite clustering or classification techniques\n   to the features to complete your recognition task.\n   See below for our examples for classification and\n   similarity detection, including an online web demo.\n\n\n\n\n\n\nWhat's in this repository?\n\n\n\n\nbatch-represent\n: Generate representations from\n  a batch of images, stored in a directory by names.\n\n\ndemos/web\n: Real-time web demo.\n\n\ndemos/compare.py\n: Demo to compare two images.\n\n\ndemos/vis-outputs.lua\n: Demo to\n  visualize the network's outputs.\n\n\ndemos/classifier.py\n: Demo to train and use classifiers.\n\n\nevaluation\n: LFW accuracy evaluation scripts.\n\n\nopenface\n: Python library code.\n\n\nmodels\n: Model directory for openface and 3rd party libraries.\n\n\ntraining\n: Scripts to train new OpenFace models.\n\n\nutil\n: Utility scripts.\n\n\n\n\nCitations\n\n\n\n\nPlease cite this repository if you use this in academic works.\n\n\n@misc{amos2015openface,\n    author       = {Amos, Brandon and Harkes, Jan and Pillai, Padmanabhan and Elgazzar, Khalid and Satyanarayanan, Mahadev},\n    title        = {OpenFace 0.1.1: Face recognition with Google's FaceNet deep neural network},\n    month        = oct,\n    year         = 2015,\n    doi          = {10.5281/zenodo.32148},\n    url          = {http://dx.doi.org/10.5281/zenodo.32148}\n}\n\n\n\n\nAcknowledgements\n\n\n\n\nThe fantastic Torch ecosystem and community.\n\n\nAlfredo Canziani's\n\n  implementation of FaceNet's loss function in\n  \ntorch-TripletEmbedding\n.\n\n\nNicholas L\u00e9onard\n\n  for quickly merging my pull requests to\n  \nnicholas-leonard/dpnn\n\n  modifying the inception layer.\n\n\nFrancisco Massa\n\n  and\n  \nAndrej Karpathy\n\n  for\n  quickly releasing \nnn.Normalize\n\n  after I expressed interest in using it.\n\n\nSoumith Chintala\n for\n  help with the \nfbcunn\n\n  example code.\n\n\nNVIDIA's academic\n  \nhardware grant program\n\n  for providing the Tesla K40 used to train the model.\n\n\nDavis King's\n \ndlib\n\n  library for face detection and alignment.\n\n\nZhuo Chen, Kiryong Ha, Wenlu Hu,\n  \nRahul Sukthankar\n, and\n  Junjue Wang for insightful discussions.\n\n\n\n\nLicensing\n\n\nThe source code and trained models \nnn4.v1.t7\n and\n\nceleb-classifier.nn4.v1.t7\n are copyright\nCarnegie Mellon University and licensed under the\n\nApache 2.0 License\n.\nPortions from the following third party sources have\nbeen modified and are included in this repository.\nThese portions are noted in the source files and are\ncopyright their respective authors with\nthe licenses listed.\n\n\n\n\n\n\n\n\nProject\n\n\nModified\n\n\nLicense\n\n\n\n\n\n\n\n\n\n\n\n\nAtcold/torch-TripletEmbedding\n\n\nNo\n\n\nMIT\n\n\n\n\n\n\n\n\nfacebook/fbnn\n\n\nYes\n\n\nBSD",
            "title": "Home"
        },
        {
            "location": "/#openface",
            "text": "Free and open source face recognition with\nGoogle's FaceNet deep neural network.    OpenFace is a Python and  Torch  implementation of the CVPR\n2015 paper FaceNet: A Unified Embedding for Face Recognition and Clustering \nby Florian Schroff, Dmitry Kalenichenko, and James Philbin at Google.\nTorch allows the network to be executed on a CPU or with CUDA.  Crafted by  Brandon Amos  in the Elijah  research group at\nCarnegie Mellon University.    The code is available on GitHub at\n   cmusatyalab/openface .  Join the\n   cmu-openface group \n  or the\n   gitter chat \n  for discussions and installation issues.  Development discussions and bugs reports are on the\n   issue tracker .    Isn't face recognition a solved problem?  No! Accuracies from research papers have just begun to surpass\nhuman accuracies on some benchmarks.\nThe accuracies of open source face recognition systems lag\nbehind the state-of-the-art.\nSee our accuracy comparisons on the famous LFW benchmark below.   Please use responsibly!  We do not support the use of this project in applications\nthat violate privacy and security.\nWe are using this to help cognitively impaired users to\nsense and understand the world around them.",
            "title": "OpenFace "
        },
        {
            "location": "/#overview",
            "text": "The following overview shows the workflow for a single input\nimage of Sylvestor Stallone from the publicly available LFW dataset .   Detect faces with a pre-trained models from\n   dlib \n  or\n   OpenCV .  Transform the face for the neural network.\n   This repository uses dlib's\n    real-time pose estimation \n   with OpenCV's\n    affine transformation \n   to try to make the eyes and bottom lip appear in\n   the same location on each image.  Use a deep neural network to represent (or embed) the face on\n   a 128-dimensional unit hypersphere.\n   The embedding is a generic representation for anybody's face.\n   Unlike other face representations, this embedding has the nice property\n   that a larger distance between two face embeddings means\n   that the faces are likely not of the same person.\n   This property makes clustering, similarity detection,\n   and classification tasks easier than other face recognition\n   techniques where the Euclidean distance between\n   features is not meaningful.  Apply your favorite clustering or classification techniques\n   to the features to complete your recognition task.\n   See below for our examples for classification and\n   similarity detection, including an online web demo.",
            "title": "Overview"
        },
        {
            "location": "/#whats-in-this-repository",
            "text": "batch-represent : Generate representations from\n  a batch of images, stored in a directory by names.  demos/web : Real-time web demo.  demos/compare.py : Demo to compare two images.  demos/vis-outputs.lua : Demo to\n  visualize the network's outputs.  demos/classifier.py : Demo to train and use classifiers.  evaluation : LFW accuracy evaluation scripts.  openface : Python library code.  models : Model directory for openface and 3rd party libraries.  training : Scripts to train new OpenFace models.  util : Utility scripts.",
            "title": "What's in this repository?"
        },
        {
            "location": "/#citations",
            "text": "Please cite this repository if you use this in academic works.  @misc{amos2015openface,\n    author       = {Amos, Brandon and Harkes, Jan and Pillai, Padmanabhan and Elgazzar, Khalid and Satyanarayanan, Mahadev},\n    title        = {OpenFace 0.1.1: Face recognition with Google's FaceNet deep neural network},\n    month        = oct,\n    year         = 2015,\n    doi          = {10.5281/zenodo.32148},\n    url          = {http://dx.doi.org/10.5281/zenodo.32148}\n}",
            "title": "Citations"
        },
        {
            "location": "/#acknowledgements",
            "text": "The fantastic Torch ecosystem and community.  Alfredo Canziani's \n  implementation of FaceNet's loss function in\n   torch-TripletEmbedding .  Nicholas L\u00e9onard \n  for quickly merging my pull requests to\n   nicholas-leonard/dpnn \n  modifying the inception layer.  Francisco Massa \n  and\n   Andrej Karpathy \n  for\n  quickly releasing  nn.Normalize \n  after I expressed interest in using it.  Soumith Chintala  for\n  help with the  fbcunn \n  example code.  NVIDIA's academic\n   hardware grant program \n  for providing the Tesla K40 used to train the model.  Davis King's   dlib \n  library for face detection and alignment.  Zhuo Chen, Kiryong Ha, Wenlu Hu,\n   Rahul Sukthankar , and\n  Junjue Wang for insightful discussions.",
            "title": "Acknowledgements"
        },
        {
            "location": "/#licensing",
            "text": "The source code and trained models  nn4.v1.t7  and celeb-classifier.nn4.v1.t7  are copyright\nCarnegie Mellon University and licensed under the Apache 2.0 License .\nPortions from the following third party sources have\nbeen modified and are included in this repository.\nThese portions are noted in the source files and are\ncopyright their respective authors with\nthe licenses listed.     Project  Modified  License       Atcold/torch-TripletEmbedding  No  MIT     facebook/fbnn  Yes  BSD",
            "title": "Licensing"
        },
        {
            "location": "/demo-1-web/",
            "text": "Demo 1: Real-Time Web Demo\n\n\nSee \nour YouTube video\n\nof using this in a real-time web application\nfor face recognition.\nThe source is available in\n\ndemos/web\n.\nThe browser portions have been tested on Google Chrome 46 in OSX.\n\n\n\n\nThis demo does the full face recognition pipeline on every frame.\nIn practice, object tracking\n\nlike dlib's\n\nshould be used once the face recognizer has predicted a face.\n\n\nTo run on your system, first follow the\n\nSetup Guide\n and make sure you can\nrun a simpler demo, like the \ncomparison demo\n.\n\n\nNext, install the requirements for the web demo with\n\n./install-deps.sh\n and \nsudo pip install -r requirements.txt\n\nfrom the \ndemos/web\n directory.\nThis is currently not included in the Docker container.\nThe application is split into a processing server and static\nweb pages that communicate via web sockets.\n\n\nStart the server with \n./demos/web/server.py\n.\nWith your client system with webcam and browser,\nyou should now be able to send a request to the websocket\nconnection with \ncurl your-server:9000\n (\nlocalhost:9000\n if running on your machine),\nwhich should inform you that it's' a WebSocket endpoint and not a web server.\nPlease check routing between your client and server if you\nget connection refused issues.\n\n\nIf you are running the server remotely (relative to your browser)\nor in a Docker container,\nchange the WebSocket connection in\n\nindex.html\n\nfrom \n127.0.0.1\n to the IP address of your server\nthat you were able to connect to with \ncurl\n.\nWith the WebSocket server running, serve the static website with\n\npython2 -m SimpleHTTPServer 8000\n from the \n/demos/web\n directory.\nYou should now be able to access the demo from your browser\nat \nhttp://your-server:8000\n, (\nhttp://localhost:8000\n if running on your machine),\nThe saved faces are only available for the browser session.",
            "title": "1 - Real-time Web"
        },
        {
            "location": "/demo-1-web/#demo-1-real-time-web-demo",
            "text": "See  our YouTube video \nof using this in a real-time web application\nfor face recognition.\nThe source is available in demos/web .\nThe browser portions have been tested on Google Chrome 46 in OSX.   This demo does the full face recognition pipeline on every frame.\nIn practice, object tracking like dlib's \nshould be used once the face recognizer has predicted a face.  To run on your system, first follow the Setup Guide  and make sure you can\nrun a simpler demo, like the  comparison demo .  Next, install the requirements for the web demo with ./install-deps.sh  and  sudo pip install -r requirements.txt \nfrom the  demos/web  directory.\nThis is currently not included in the Docker container.\nThe application is split into a processing server and static\nweb pages that communicate via web sockets.  Start the server with  ./demos/web/server.py .\nWith your client system with webcam and browser,\nyou should now be able to send a request to the websocket\nconnection with  curl your-server:9000  ( localhost:9000  if running on your machine),\nwhich should inform you that it's' a WebSocket endpoint and not a web server.\nPlease check routing between your client and server if you\nget connection refused issues.  If you are running the server remotely (relative to your browser)\nor in a Docker container,\nchange the WebSocket connection in index.html \nfrom  127.0.0.1  to the IP address of your server\nthat you were able to connect to with  curl .\nWith the WebSocket server running, serve the static website with python2 -m SimpleHTTPServer 8000  from the  /demos/web  directory.\nYou should now be able to access the demo from your browser\nat  http://your-server:8000 , ( http://localhost:8000  if running on your machine),\nThe saved faces are only available for the browser session.",
            "title": "Demo 1: Real-Time Web Demo"
        },
        {
            "location": "/demo-2-comparison/",
            "text": "Demo 2: Comparing two images\n\n\nThe \ncomparison demo\n outputs the predicted similarity\nscore of two faces by computing the squared L2 distance between\ntheir representations.\nA lower score indicates two faces are more likely of the same person.\nSince the representations are on the unit hypersphere, the\nscores range from 0 (the same picture) to 4.0.\nThe following distances between images of John Lennon and\nEric Clapton were generated with\n\n./demos/compare.py images/examples/{lennon*,clapton*}\n.\n\n\n\n\n\n\n\n\nLennon 1\n\n\nLennon 2\n\n\nClapton 1\n\n\nClapton 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following table shows that a distance threshold of \n0.5\n would\ndistinguish these two people.\nIn practice, further experimentation should be done on the distance threshold.\nOn our LFW experiments, the mean threshold across multiple\nexperiments is 0.71 \u00b1 0.027,\nsee \naccuracies.txt\n.\n\n\n\n\n\n\n\n\nImage 1\n\n\nImage 2\n\n\nDistance\n\n\n\n\n\n\n\n\n\n\nLennon 1\n\n\nLennon 2\n\n\n0.310\n\n\n\n\n\n\nLennon 1\n\n\nClapton 1\n\n\n1.241\n\n\n\n\n\n\nLennon 1\n\n\nClapton 2\n\n\n1.056\n\n\n\n\n\n\nLennon 2\n\n\nClapton 1\n\n\n1.386\n\n\n\n\n\n\nLennon 2\n\n\nClapton 2\n\n\n1.073\n\n\n\n\n\n\nClapton 1\n\n\nClapton 2\n\n\n0.259",
            "title": "2 - Comparison"
        },
        {
            "location": "/demo-2-comparison/#demo-2-comparing-two-images",
            "text": "The  comparison demo  outputs the predicted similarity\nscore of two faces by computing the squared L2 distance between\ntheir representations.\nA lower score indicates two faces are more likely of the same person.\nSince the representations are on the unit hypersphere, the\nscores range from 0 (the same picture) to 4.0.\nThe following distances between images of John Lennon and\nEric Clapton were generated with ./demos/compare.py images/examples/{lennon*,clapton*} .     Lennon 1  Lennon 2  Clapton 1  Clapton 2             The following table shows that a distance threshold of  0.5  would\ndistinguish these two people.\nIn practice, further experimentation should be done on the distance threshold.\nOn our LFW experiments, the mean threshold across multiple\nexperiments is 0.71 \u00b1 0.027,\nsee  accuracies.txt .     Image 1  Image 2  Distance      Lennon 1  Lennon 2  0.310    Lennon 1  Clapton 1  1.241    Lennon 1  Clapton 2  1.056    Lennon 2  Clapton 1  1.386    Lennon 2  Clapton 2  1.073    Clapton 1  Clapton 2  0.259",
            "title": "Demo 2: Comparing two images"
        },
        {
            "location": "/demo-3-classifier/",
            "text": "Demo 3: Training a Classifier\n\n\nOpenFace's core provides a feature extraction method to\nobtain a low-dimensional representation of any face.\n\ndemos/classifier.py\n\nshows a demo of how these representations can be\nused to create a face classifier.\n\n\nThere is a distinction between training the deep neural network (DNN)\nmodel for feature representation\nand training a model for classifying people with the DNN model.\nThis shows how to use a pre-trained DNN model to train and use\na classification model.\n\n\nCreating a Classification Model\n\n\n1. Create raw image directory.\n\n\nCreate a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as \njpg\n or \npng\n and have\na lowercase extension.\n\n\n$ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png\n\n\n\n\n2. Preprocess the raw images\n\n\nChange \n8\n to however many\nseparate processes you want to run:\n\nfor N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align affine <path-to-aligned-data> --size 96 &; done\n.\n\n\n3. Create the Classification Model\n\n\nUse \n./demos/classifier.py train <path-to-aligned-data>\n to produce\nthe classification model which is an SVM saved to disk as\na Python pickle.\n\n\nTraining uses \nscikit-learn\n to perform\na grid search over SVM parameters.\nFor 1000's of images, training the SVMs takes seconds.\nOur trained model obtains 87% accuracy on this set of data.\n\n\nClassifying New Images\n\n\nWe have released a \nceleb-classifier.nn4.v1.pkl\n classification model\nthat is trained on about 6000 total images of the following people,\nwhich are the people with the most images in our dataset.\nClassifiers can be created with far less images per\nperson.\n\n\n\n\nAmerica Ferrera\n\n\nAmy Adams\n\n\nAnne Hathaway\n\n\nBen Stiller\n\n\nBradley Cooper\n\n\nDavid Boreanaz\n\n\nEmily Deschanel\n\n\nEva Longoria\n\n\nJon Hamm\n\n\nSteve Carell\n\n\n\n\nFor an example, consider the following small set of images\nthe model has no knowledge of.\nFor an unknown person, a prediction still needs to be made, but\nthe confidence score is usually lower.\n\n\nRun the classifier on your images with:\n\n\n./demos/classifier.py infer ./models/openface/celeb-classifier.nn4.v1.pkl ./your-image.png\n\n\n\n\n\n\n\n\n\n\nPerson\n\n\nImage\n\n\nPrediction\n\n\nConfidence\n\n\n\n\n\n\n\n\n\n\nCarell\n\n\n\n\nSteveCarell\n\n\n0.78\n\n\n\n\n\n\nAdams\n\n\n\n\nAmyAdams\n\n\n0.87\n\n\n\n\n\n\nLennon 1 (Unknown)\n\n\n\n\nDavidBoreanaz\n\n\n0.28\n\n\n\n\n\n\nLennon 2 (Unknown)\n\n\n\n\nDavidBoreanaz\n\n\n0.56",
            "title": "3 - Classifier"
        },
        {
            "location": "/demo-3-classifier/#demo-3-training-a-classifier",
            "text": "OpenFace's core provides a feature extraction method to\nobtain a low-dimensional representation of any face. demos/classifier.py \nshows a demo of how these representations can be\nused to create a face classifier.  There is a distinction between training the deep neural network (DNN)\nmodel for feature representation\nand training a model for classifying people with the DNN model.\nThis shows how to use a pre-trained DNN model to train and use\na classification model.",
            "title": "Demo 3: Training a Classifier"
        },
        {
            "location": "/demo-3-classifier/#creating-a-classification-model",
            "text": "1. Create raw image directory.  Create a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as  jpg  or  png  and have\na lowercase extension.  $ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png",
            "title": "Creating a Classification Model"
        },
        {
            "location": "/demo-3-classifier/#2-preprocess-the-raw-images",
            "text": "Change  8  to however many\nseparate processes you want to run: for N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align affine <path-to-aligned-data> --size 96 &; done .",
            "title": "2. Preprocess the raw images"
        },
        {
            "location": "/demo-3-classifier/#3-create-the-classification-model",
            "text": "Use  ./demos/classifier.py train <path-to-aligned-data>  to produce\nthe classification model which is an SVM saved to disk as\na Python pickle.  Training uses  scikit-learn  to perform\na grid search over SVM parameters.\nFor 1000's of images, training the SVMs takes seconds.\nOur trained model obtains 87% accuracy on this set of data.",
            "title": "3. Create the Classification Model"
        },
        {
            "location": "/demo-3-classifier/#classifying-new-images",
            "text": "We have released a  celeb-classifier.nn4.v1.pkl  classification model\nthat is trained on about 6000 total images of the following people,\nwhich are the people with the most images in our dataset.\nClassifiers can be created with far less images per\nperson.   America Ferrera  Amy Adams  Anne Hathaway  Ben Stiller  Bradley Cooper  David Boreanaz  Emily Deschanel  Eva Longoria  Jon Hamm  Steve Carell   For an example, consider the following small set of images\nthe model has no knowledge of.\nFor an unknown person, a prediction still needs to be made, but\nthe confidence score is usually lower.  Run the classifier on your images with:  ./demos/classifier.py infer ./models/openface/celeb-classifier.nn4.v1.pkl ./your-image.png     Person  Image  Prediction  Confidence      Carell   SteveCarell  0.78    Adams   AmyAdams  0.87    Lennon 1 (Unknown)   DavidBoreanaz  0.28    Lennon 2 (Unknown)   DavidBoreanaz  0.56",
            "title": "Classifying New Images"
        },
        {
            "location": "/accuracy/",
            "text": "Cool demos, but I want numbers. What's the accuracy?\n\n\nEven though the public datasets we trained on have orders of magnitude less data\nthan private industry datasets, the accuracy is remarkably high\non the standard\n\nLFW\n\nbenchmark.\nWe had to fallback to using the deep funneled versions for\n152 of 13233 images because dlib failed to detect a face or landmarks.\nWe obtain a mean accuracy of 0.8483 \u00b1 0.0172 with an AUC of 0.923.\nFor comparison, training with Google-scale data results in an\naccuracy of .9963 \u00b1 0.009.\n\n\n\n\nThis can be generated with the following commands from the root \nopenface\n\ndirectory, assuming you have downloaded and placed the raw and\ndeep funneled LFW data from \nhere\n\nin \n./data/lfw/raw\n and \n./data/lfw/deepfunneled\n.\n\n\n\n\nInstall prerequisites as below.\n\n\nPreprocess the raw \nlfw\n images, change \n8\n to however many\n   separate processes you want to run:\n   \nfor N in {1..8}; do ./util/align-dlib.py data/lfw/raw align affine data/lfw/dlib-affine-sz:96 --size 96 &; done\n.\n   Fallback to deep funneled versions for images that dlib failed\n   to align:\n   \n./util/align-dlib.py data/lfw/raw align affine data/lfw/dlib-affine-sz:96 --size 96 --fallbackLfw data/lfw/deepfunneled\n\n\nGenerate representations with \n./batch-represent/main.lua -outDir evaluation/lfw.nn4.v1.reps -model models/openface/nn4.v1.t7 -data data/lfw/dlib-affine-sz:96\n\n\nGenerate the ROC curve from the \nevaluation\n directory with \n./lfw-roc.py --workDir lfw.nn4.v1.reps\n.\n   This creates \nroc.pdf\n in the \nlfw.nn4.v1.reps\n directory.\n\n\n\n\n\n\nIf you're interested in higher accuracy open source code, see:\n\n\n\n\nOxford's VGG Face Descriptor\n,\n  which is licensed for non-commercial research purposes.\n  They've released their softmax network, which obtains .9727 accuracy\n  on the LFW and will release their triplet network (0.9913 accuracy)\n  and data soon.\n\n\n\n\nTheir softmax model doesn't embed features like FaceNet,\n  which makes tasks like classification and clustering more difficult.\n  Their triplet model hasn't yet been released, but will provide\n  embeddings similar to FaceNet.\n  The triplet model will be supported by OpenFace once it's released.\n2. \nAlfredXiangWu/face_verification_experiment\n,\n  which uses Caffe and doesn't yet have a license.\n  The accuracy on the LFW is .9777.\n  This model doesn't embed features like FaceNet,\n  which makes tasks like classification and clustering more difficult.",
            "title": "Accuracy"
        },
        {
            "location": "/accuracy/#cool-demos-but-i-want-numbers-whats-the-accuracy",
            "text": "Even though the public datasets we trained on have orders of magnitude less data\nthan private industry datasets, the accuracy is remarkably high\non the standard LFW \nbenchmark.\nWe had to fallback to using the deep funneled versions for\n152 of 13233 images because dlib failed to detect a face or landmarks.\nWe obtain a mean accuracy of 0.8483 \u00b1 0.0172 with an AUC of 0.923.\nFor comparison, training with Google-scale data results in an\naccuracy of .9963 \u00b1 0.009.   This can be generated with the following commands from the root  openface \ndirectory, assuming you have downloaded and placed the raw and\ndeep funneled LFW data from  here \nin  ./data/lfw/raw  and  ./data/lfw/deepfunneled .   Install prerequisites as below.  Preprocess the raw  lfw  images, change  8  to however many\n   separate processes you want to run:\n    for N in {1..8}; do ./util/align-dlib.py data/lfw/raw align affine data/lfw/dlib-affine-sz:96 --size 96 &; done .\n   Fallback to deep funneled versions for images that dlib failed\n   to align:\n    ./util/align-dlib.py data/lfw/raw align affine data/lfw/dlib-affine-sz:96 --size 96 --fallbackLfw data/lfw/deepfunneled  Generate representations with  ./batch-represent/main.lua -outDir evaluation/lfw.nn4.v1.reps -model models/openface/nn4.v1.t7 -data data/lfw/dlib-affine-sz:96  Generate the ROC curve from the  evaluation  directory with  ./lfw-roc.py --workDir lfw.nn4.v1.reps .\n   This creates  roc.pdf  in the  lfw.nn4.v1.reps  directory.    If you're interested in higher accuracy open source code, see:   Oxford's VGG Face Descriptor ,\n  which is licensed for non-commercial research purposes.\n  They've released their softmax network, which obtains .9727 accuracy\n  on the LFW and will release their triplet network (0.9913 accuracy)\n  and data soon.   Their softmax model doesn't embed features like FaceNet,\n  which makes tasks like classification and clustering more difficult.\n  Their triplet model hasn't yet been released, but will provide\n  embeddings similar to FaceNet.\n  The triplet model will be supported by OpenFace once it's released.\n2.  AlfredXiangWu/face_verification_experiment ,\n  which uses Caffe and doesn't yet have a license.\n  The accuracy on the LFW is .9777.\n  This model doesn't embed features like FaceNet,\n  which makes tasks like classification and clustering more difficult.",
            "title": "Cool demos, but I want numbers. What's the accuracy?"
        },
        {
            "location": "/usage/",
            "text": "Usage\n\n\nExisting Models\n\n\nSee \nthe image comparison demo\n for a complete example\nwritten in Python using a naive Torch subprocess to process the faces.\n\n\nimport openface\nfrom openface.alignment import NaiveDlib # Depends on dlib.\n\n# `args` are parsed command-line arguments.\n\nalign = NaiveDlib(args.dlibFaceMean, args.dlibFacePredictor)\nnet = openface.TorchWrap(args.networkModel, imgDim=args.imgDim, cuda=args.cuda)\n\n# `img` is a numpy matrix containing the RGB pixels of the image.\nbb = align.getLargestFaceBoundingBox(img)\nalignedFace = align.alignImg(\"affine\", args.imgDim, img, bb)\nrep1 = net.forwardImage(alignedFace)\n\n# `rep2` obtained similarly.\nd = rep1 - rep2\ndistance = np.dot(d, d)",
            "title": "Usage"
        },
        {
            "location": "/usage/#usage",
            "text": "",
            "title": "Usage"
        },
        {
            "location": "/usage/#existing-models",
            "text": "See  the image comparison demo  for a complete example\nwritten in Python using a naive Torch subprocess to process the faces.  import openface\nfrom openface.alignment import NaiveDlib # Depends on dlib.\n\n# `args` are parsed command-line arguments.\n\nalign = NaiveDlib(args.dlibFaceMean, args.dlibFacePredictor)\nnet = openface.TorchWrap(args.networkModel, imgDim=args.imgDim, cuda=args.cuda)\n\n# `img` is a numpy matrix containing the RGB pixels of the image.\nbb = align.getLargestFaceBoundingBox(img)\nalignedFace = align.alignImg(\"affine\", args.imgDim, img, bb)\nrep1 = net.forwardImage(alignedFace)\n\n# `rep2` obtained similarly.\nd = rep1 - rep2\ndistance = np.dot(d, d)",
            "title": "Existing Models"
        },
        {
            "location": "/models/",
            "text": "Model Definitions\n\n\nModel definitions should be kept in \nmodels/openface\n,\nwhere we have provided definitions of the \nNN2\n\nand \nnn4\n as described in the paper,\nbut with batch normalization and no normalization in the lower layers.\nThe inception layers are introduced  in\n\nGoing Deeper with Convolutions\n\nby Christian Szegedy et al.\n\n\nPre-trained Models\n\n\nPre-trained models are versioned and should be released with\na corresponding model definition.\nWe currently only provide a pre-trained model for \nnn4.v1\n\nbecause we have limited access to large-scale face recognition\ndatasets.\n\n\nnn4.v1\n\n\nThis model has been trained by combining the two largest (of August 2015)\npublicly-available face recognition datasets based on names:\n\nFaceScrub\n\nand \nCASIA-WebFace\n.\nThis model was trained for about 300 hours on a Tesla K40 GPU.\n\n\nThe following plot shows the triplet loss on the training\nand test set.\nEach training epoch is defined to be 1000 minibatches, where\neach minibatch processes 100 triplets.\nEach testing epoch is defined to be 300 minibatches,\nwhere each minibatch processes 100 triplets.\nSemi-hard triplets are used on the training set, and\nrandom triplets are used on the testing set.\nOur \nnn4.v1\n model is from epoch 177.\n\n\nThe LFW section above shows that this model obtains a mean\naccuracy of 0.8483 \u00b1 0.0172 with an AUC of 0.923.",
            "title": "Models"
        },
        {
            "location": "/models/#model-definitions",
            "text": "Model definitions should be kept in  models/openface ,\nwhere we have provided definitions of the  NN2 \nand  nn4  as described in the paper,\nbut with batch normalization and no normalization in the lower layers.\nThe inception layers are introduced  in Going Deeper with Convolutions \nby Christian Szegedy et al.",
            "title": "Model Definitions"
        },
        {
            "location": "/models/#pre-trained-models",
            "text": "Pre-trained models are versioned and should be released with\na corresponding model definition.\nWe currently only provide a pre-trained model for  nn4.v1 \nbecause we have limited access to large-scale face recognition\ndatasets.",
            "title": "Pre-trained Models"
        },
        {
            "location": "/models/#nn4v1",
            "text": "This model has been trained by combining the two largest (of August 2015)\npublicly-available face recognition datasets based on names: FaceScrub \nand  CASIA-WebFace .\nThis model was trained for about 300 hours on a Tesla K40 GPU.  The following plot shows the triplet loss on the training\nand test set.\nEach training epoch is defined to be 1000 minibatches, where\neach minibatch processes 100 triplets.\nEach testing epoch is defined to be 300 minibatches,\nwhere each minibatch processes 100 triplets.\nSemi-hard triplets are used on the training set, and\nrandom triplets are used on the testing set.\nOur  nn4.v1  model is from epoch 177.  The LFW section above shows that this model obtains a mean\naccuracy of 0.8483 \u00b1 0.0172 with an AUC of 0.923.",
            "title": "nn4.v1"
        },
        {
            "location": "/training-new-models/",
            "text": "Training new neural network models\n\n\nWe have also released our deep neural network (DNN)\ntraining infrastructure to promote an open ecosystem and enable quicker\nbootstrapping for new research and development.\n\n\nThere is a distinction between training the DNN model for feature representation\nand training a model for classifying people with the DNN model.\nIf you're interested in creating a new classifier,\nsee \nDemo 3\n.\n\n\nTraining a new DNN model is for advanced users and should be done\nwith large datasets (>500k images) to improve the feature representation,\nnot for classification.\n\n\nWarning: Training is computationally and memory expensive and takes a\nfew weeks on our Tesla K40 GPU.\nBecause of this, the training code assumes CUDA is installed.\n\n\nA rough overview of training is:\n\n\n1. Create raw image directory.\n\n\nCreate a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as \njpg\n or \npng\n and have\na lowercase extension.\n\n\n$ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png\n\n\n\n\n2. Preprocess the raw images\n\n\nChange \n8\n to however many\nseparate processes you want to run:\n\nfor N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align affine <path-to-aligned-data> --size 96 &; done\n.\nPrune out directories with less than N (I use 10) images\nper class with \n./util/prune-dataset.py <path-to-aligned-data> --numImagesThreshold <N>\n and\nthen split the dataset into \ntrain\n and \nval\n subdirectories\nwith \n./util/create-train-val-split.py <path-to-aligned-data> <validation-ratio>\n.\n\n\n3. Train the model\n\n\nRun \ntraining/main.lua\n to start training the model.\nEdit the dataset options in \ntraining/opts.lua\n or\npass them as command-line parameters.\nThis will output the loss and in-progress models to \ntraining/work\n.\nThe default minibatch size (parameter \n-batchSize\n) is 100 and requires\nabout 10GB of GPU memory.\n\n\nWarning: Metadata about the on-disk data is cached in\n\ntraining/work/{train,test}Cache.t7\n and assumes\nthe data directory does not change.\nIf your data directory changes, delete these\nfiles so they will be regenerated.\n\n\nStopping and starting training\n\n\nModels are saved in the \nwork\n directory after every epoch.\nIf the training process is killed, it can be resumed from\nthe last saved model with the \n-retrain\n option.\nAlso pass a different \n-manualSeed\n so a different image\nsequence is sampled and correctly set \n-epochNumber\n.\n\n\n4. Analyze training\n\n\nVisualize the loss with \ntraining/plot-loss.py\n.",
            "title": "DNN Training"
        },
        {
            "location": "/training-new-models/#training-new-neural-network-models",
            "text": "We have also released our deep neural network (DNN)\ntraining infrastructure to promote an open ecosystem and enable quicker\nbootstrapping for new research and development.  There is a distinction between training the DNN model for feature representation\nand training a model for classifying people with the DNN model.\nIf you're interested in creating a new classifier,\nsee  Demo 3 .  Training a new DNN model is for advanced users and should be done\nwith large datasets (>500k images) to improve the feature representation,\nnot for classification.  Warning: Training is computationally and memory expensive and takes a\nfew weeks on our Tesla K40 GPU.\nBecause of this, the training code assumes CUDA is installed.  A rough overview of training is:",
            "title": "Training new neural network models"
        },
        {
            "location": "/training-new-models/#1-create-raw-image-directory",
            "text": "Create a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as  jpg  or  png  and have\na lowercase extension.  $ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png",
            "title": "1. Create raw image directory."
        },
        {
            "location": "/training-new-models/#2-preprocess-the-raw-images",
            "text": "Change  8  to however many\nseparate processes you want to run: for N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align affine <path-to-aligned-data> --size 96 &; done .\nPrune out directories with less than N (I use 10) images\nper class with  ./util/prune-dataset.py <path-to-aligned-data> --numImagesThreshold <N>  and\nthen split the dataset into  train  and  val  subdirectories\nwith  ./util/create-train-val-split.py <path-to-aligned-data> <validation-ratio> .",
            "title": "2. Preprocess the raw images"
        },
        {
            "location": "/training-new-models/#3-train-the-model",
            "text": "Run  training/main.lua  to start training the model.\nEdit the dataset options in  training/opts.lua  or\npass them as command-line parameters.\nThis will output the loss and in-progress models to  training/work .\nThe default minibatch size (parameter  -batchSize ) is 100 and requires\nabout 10GB of GPU memory.  Warning: Metadata about the on-disk data is cached in training/work/{train,test}Cache.t7  and assumes\nthe data directory does not change.\nIf your data directory changes, delete these\nfiles so they will be regenerated.  Stopping and starting training  Models are saved in the  work  directory after every epoch.\nIf the training process is killed, it can be resumed from\nthe last saved model with the  -retrain  option.\nAlso pass a different  -manualSeed  so a different image\nsequence is sampled and correctly set  -epochNumber .",
            "title": "3. Train the model"
        },
        {
            "location": "/training-new-models/#4-analyze-training",
            "text": "Visualize the loss with  training/plot-loss.py .",
            "title": "4. Analyze training"
        },
        {
            "location": "/visualizations/",
            "text": "Visualizing representations with t-SNE\n\n\nt-SNE\n is a dimensionality\nreduction technique that can be used to visualize the\n128-dimensional features OpenFace produces.\nThe following shows the visualization of the three people\nin the training and testing dataset with the most images.\n\n\nTraining\n\n\n\n\nTesting\n\n\n\n\nThese can be generated with the following commands from the root\n\nopenface\n directory.\n\n\n\n\nInstall prerequisites as below.\n\n\nPreprocess the raw \nlfw\n images, change \n8\n to however many\n   separate processes you want to run:\n   \nfor N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align affine <path-to-aligned-data> --size 96 &; done\n.\n\n\nGenerate representations with \n./batch-represent/main.lua -outDir <feature-directory (to be created)> -model models/openface/nn4.v1.t7 -data <path-to-aligned-data>\n\n\nGenerate t-SNE visualization with \n./util/tsne.py <feature-directory> --names <name 1> ... <name n>\n\n   This creates \ntsne.pdf\n in \n<feature-directory>\n.\n\n\n\n\nVisualizing layer outputs\n\n\nVisualizing the output feature maps of each layer\nis sometimes helpful to understand what features\nthe network has learned to extract.\nWith faces, the locations of the eyes, nose, and\nmouth should play an important role.\n\n\ndemos/vis-outputs.lua\n\noutputs the feature maps from an aligned image.\nThe following shows the first 39 filters of the\nfirst convolutional layer on two images\nof John Lennon.",
            "title": "Visualizations"
        },
        {
            "location": "/visualizations/#visualizing-representations-with-t-sne",
            "text": "t-SNE  is a dimensionality\nreduction technique that can be used to visualize the\n128-dimensional features OpenFace produces.\nThe following shows the visualization of the three people\nin the training and testing dataset with the most images.  Training   Testing   These can be generated with the following commands from the root openface  directory.   Install prerequisites as below.  Preprocess the raw  lfw  images, change  8  to however many\n   separate processes you want to run:\n    for N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align affine <path-to-aligned-data> --size 96 &; done .  Generate representations with  ./batch-represent/main.lua -outDir <feature-directory (to be created)> -model models/openface/nn4.v1.t7 -data <path-to-aligned-data>  Generate t-SNE visualization with  ./util/tsne.py <feature-directory> --names <name 1> ... <name n> \n   This creates  tsne.pdf  in  <feature-directory> .",
            "title": "Visualizing representations with t-SNE"
        },
        {
            "location": "/visualizations/#visualizing-layer-outputs",
            "text": "Visualizing the output feature maps of each layer\nis sometimes helpful to understand what features\nthe network has learned to extract.\nWith faces, the locations of the eyes, nose, and\nmouth should play an important role.  demos/vis-outputs.lua \noutputs the feature maps from an aligned image.\nThe following shows the first 39 filters of the\nfirst convolutional layer on two images\nof John Lennon.",
            "title": "Visualizing layer outputs"
        },
        {
            "location": "/setup/",
            "text": "Setup\n\n\nThe following instructions are for Linux and OSX only.\nPlease contribute modifications and build instructions if you\nare interested in running this on other operating systems.\n\n\nWe strongly recommend using the \nDocker\n\ncontainer unless you are experienced with building\nLinux software from source.\n\n\nAlso note that in OSX, you may have to change the hashbangs\nfrom \npython2\n to \npython\n.\n\n\nWarning for architectures other than 64-bit x86\n\n\nSee \n#42\n.\n\n\nCheck out git submodules\n\n\nClone with \n--recursive\n or run \ngit submodule init && git submodule update\n\nafter checking out.\n\n\nDownload the models\n\n\nRun \nmodels/get-models.sh\n\nto download pre-trained OpenFace\nmodels on the combined CASIA-WebFace and FaceScrub database.\nThis also downloads dlib's pre-trained model for face landmark detection.\nThis will incur about 500MB of network traffic for the compressed\nmodels that will decompress to about 1GB on disk.\n\n\nBe sure the md5 checksums match the following.\nUse \nmd5sum\n in Linux and \nmd5\n in OSX.\n\n\nopenface(master)$ md5sum models/{dlib/*.dat,openface/*.{pkl,t7}}\n73fde5e05226548677a050913eed4e04  models/dlib/shape_predictor_68_face_landmarks.dat\nc0675d57dc976df601b085f4af67ecb9  models/openface/celeb-classifier.nn4.v1.pkl\na59a5ec1938370cd401b257619848960  models/openface/nn4.v1.t7\n\n\n\n\nWith Docker\n\n\nThis repo can be deployed as a container with \nDocker\n\nfor CPU mode.\nBe sure you have checked out the submodules and downloaded\nthe models as described above.\nDepending on your Docker configuration, you may need to\nrun the docker commands as root.\n\n\nTo use, place your images in \nopenface\n on your host and\naccess them from the shared Docker directory.\n\n\ndocker build -t openface ./docker\ndocker run -p 9000:9000 -t -i -v $PWD:/openface openface /bin/bash\ncd /openface\n./demos/compare.py images/examples/{lennon*,clapton*}\n\n\n\n\nDocker in OSX\n\n\nIn OSX, follow the\n\nDocker Mac OSX Installation Guide\n\nand start a docker machine and connect your shell to it\nbefore trying to build the container.\nIn the simplest case, this can be done with:\n\n\ndocker-machine create --driver virtualbox default\neval $(docker-machine env default)\n\n\n\n\nBy hand\n\n\nBe sure you have checked out the submodules and downloaded the models as\ndescribed above.\nSee the\n\nDockerfile\n\nas a reference.\n\n\nThis project uses \npython2\n because of the \nopencv\n\nand \ndlib\n dependencies.\nInstall the packages the Dockerfile uses with your package manager.\nWith \npip2\n, install \nnumpy\n, \npandas\n, \nscipy\n, \nscikit-learn\n, and \nscikit-image\n.\n\n\nNext, manually install the following.\n\n\nOpenCV\n\n\nDownload \nOpenCV 2.4.11\n\nand follow their\n\nbuild instructions\n.\n\n\ndlib\n\n\ndlib can alternatively by installed from \npypi\n,\nbut might be slower than building manually because they are not\ncompiled with AVX support.\n\n\ndlib requires boost libraries to be installed.\n\n\nTo build manually, start by\ndownloading\n\ndlib v18.16\n,\nthen:\n\n\nmkdir -p ~/src\ncd ~/src\ntar xf dlib-18.16.tar.bz2\ncd dlib-18.16/python_examples\nmkdir build\ncd build\ncmake ../../tools/python\ncmake --build . --config Release\ncp dlib.so ..\n\n\n\n\nAt this point, you should be able to start your \npython2\n\ninterpreter and successfully run \nimport cv2; import dlib\n.\n\n\nIn OSX, you may get a \nFatal Python error: PyThreadState_Get: no current thread\n.\nYou may be able to resolve by rebuilding \npython\n and \nboost-python\n\nas reported in \n#21\n,\nbut please file a new issue with us or \ndlib\n\nif you are unable to resolve this.\n\n\nTorch\n\n\nInstall \nTorch\n from the instructions on their website\nand install the \ndpnn\n\nand \nnn\n libraries with\n\nluarocks install dpnn\n and \nluarocks install nn\n.\n\n\nIf you want CUDA support, also install\n\ncudnn.torch\n.\n\n\nAt this point, the command-line program \nth\n should\nbe available in your shell.",
            "title": "Setup"
        },
        {
            "location": "/setup/#setup",
            "text": "The following instructions are for Linux and OSX only.\nPlease contribute modifications and build instructions if you\nare interested in running this on other operating systems.  We strongly recommend using the  Docker \ncontainer unless you are experienced with building\nLinux software from source.  Also note that in OSX, you may have to change the hashbangs\nfrom  python2  to  python .",
            "title": "Setup"
        },
        {
            "location": "/setup/#warning-for-architectures-other-than-64-bit-x86",
            "text": "See  #42 .",
            "title": "Warning for architectures other than 64-bit x86"
        },
        {
            "location": "/setup/#check-out-git-submodules",
            "text": "Clone with  --recursive  or run  git submodule init && git submodule update \nafter checking out.",
            "title": "Check out git submodules"
        },
        {
            "location": "/setup/#download-the-models",
            "text": "Run  models/get-models.sh \nto download pre-trained OpenFace\nmodels on the combined CASIA-WebFace and FaceScrub database.\nThis also downloads dlib's pre-trained model for face landmark detection.\nThis will incur about 500MB of network traffic for the compressed\nmodels that will decompress to about 1GB on disk.  Be sure the md5 checksums match the following.\nUse  md5sum  in Linux and  md5  in OSX.  openface(master)$ md5sum models/{dlib/*.dat,openface/*.{pkl,t7}}\n73fde5e05226548677a050913eed4e04  models/dlib/shape_predictor_68_face_landmarks.dat\nc0675d57dc976df601b085f4af67ecb9  models/openface/celeb-classifier.nn4.v1.pkl\na59a5ec1938370cd401b257619848960  models/openface/nn4.v1.t7",
            "title": "Download the models"
        },
        {
            "location": "/setup/#with-docker",
            "text": "This repo can be deployed as a container with  Docker \nfor CPU mode.\nBe sure you have checked out the submodules and downloaded\nthe models as described above.\nDepending on your Docker configuration, you may need to\nrun the docker commands as root.  To use, place your images in  openface  on your host and\naccess them from the shared Docker directory.  docker build -t openface ./docker\ndocker run -p 9000:9000 -t -i -v $PWD:/openface openface /bin/bash\ncd /openface\n./demos/compare.py images/examples/{lennon*,clapton*}  Docker in OSX  In OSX, follow the Docker Mac OSX Installation Guide \nand start a docker machine and connect your shell to it\nbefore trying to build the container.\nIn the simplest case, this can be done with:  docker-machine create --driver virtualbox default\neval $(docker-machine env default)",
            "title": "With Docker"
        },
        {
            "location": "/setup/#by-hand",
            "text": "Be sure you have checked out the submodules and downloaded the models as\ndescribed above.\nSee the Dockerfile \nas a reference.  This project uses  python2  because of the  opencv \nand  dlib  dependencies.\nInstall the packages the Dockerfile uses with your package manager.\nWith  pip2 , install  numpy ,  pandas ,  scipy ,  scikit-learn , and  scikit-image .  Next, manually install the following.  OpenCV  Download  OpenCV 2.4.11 \nand follow their build instructions .  dlib  dlib can alternatively by installed from  pypi ,\nbut might be slower than building manually because they are not\ncompiled with AVX support.  dlib requires boost libraries to be installed.  To build manually, start by\ndownloading dlib v18.16 ,\nthen:  mkdir -p ~/src\ncd ~/src\ntar xf dlib-18.16.tar.bz2\ncd dlib-18.16/python_examples\nmkdir build\ncd build\ncmake ../../tools/python\ncmake --build . --config Release\ncp dlib.so ..  At this point, you should be able to start your  python2 \ninterpreter and successfully run  import cv2; import dlib .  In OSX, you may get a  Fatal Python error: PyThreadState_Get: no current thread .\nYou may be able to resolve by rebuilding  python  and  boost-python \nas reported in  #21 ,\nbut please file a new issue with us or  dlib \nif you are unable to resolve this.  Torch  Install  Torch  from the instructions on their website\nand install the  dpnn \nand  nn  libraries with luarocks install dpnn  and  luarocks install nn .  If you want CUDA support, also install cudnn.torch .  At this point, the command-line program  th  should\nbe available in your shell.",
            "title": "By hand"
        }
    ]
}