{
    "docs": [
        {
            "location": "/",
            "text": "OpenFace \n\n\n\n\nFree and open source face recognition with\ndeep neural networks.\n\n\n\n\n\n\nNews\n\n\n\n\n2016-01-19: OpenFace 0.2.0 released!\n  See \nthis blog post\n\n  for more details.\n\n\n\n\n\n\nOpenFace is a Python and \nTorch\n implementation of\nface recognition with deep neural networks and is based on\nthe CVPR 2015 paper\n\nFaceNet: A Unified Embedding for Face Recognition and Clustering\n\nby Florian Schroff, Dmitry Kalenichenko, and James Philbin at Google.\nTorch allows the network to be executed on a CPU or with CUDA.\n\n\nCrafted by \nBrandon Amos\n in\n\nSatya's\n research group at\nCarnegie Mellon University.\n\n\n\n\n\n\nThe code is available on GitHub at\n  \ncmusatyalab/openface\n.\n\n\nAPI Documentation\n\n\nJoin the\n  \ncmu-openface group\n\n  or the\n  \ngitter chat\n\n  for discussions and installation issues.\n\n\nDevelopment discussions and bugs reports are on the\n  \nissue tracker\n.\n\n\n\n\n\n\nThis research was supported by the National Science Foundation (NSF)\nunder grant number CNS-1518865.  Additional support\nwas provided by the Intel Corporation, Google, Vodafone, NVIDIA, and the\nConklin Kistler family fund.  Any opinions, findings, conclusions or\nrecommendations expressed in this material are those of the authors\nand should not be attributed to their employers or funding sources.\n\n\n\n\nIsn't face recognition a solved problem?\n\n\nNo! Accuracies from research papers have just begun to surpass\nhuman accuracies on some benchmarks.\nThe accuracies of open source face recognition systems lag\nbehind the state-of-the-art.\nSee \nour accuracy comparisons\n\non the famous LFW benchmark.\n\n\n\n\nPlease use responsibly!\n\n\nWe do not support the use of this project in applications\nthat violate privacy and security.\nWe are using this to help cognitively impaired users\nsense and understand the world around them.\n\n\n\n\nOverview\n\n\nThe following overview shows the workflow for a single input\nimage of Sylvestor Stallone from the publicly available\n\nLFW dataset\n.\n\n\n\n\nDetect faces with a pre-trained models from\n  \ndlib\n\n  or\n  \nOpenCV\n.\n\n\nTransform the face for the neural network.\n   This repository uses dlib's\n   \nreal-time pose estimation\n\n   with OpenCV's\n   \naffine transformation\n\n   to try to make the eyes and bottom lip appear in\n   the same location on each image.\n\n\nUse a deep neural network to represent (or embed) the face on\n   a 128-dimensional unit hypersphere.\n   The embedding is a generic representation for anybody's face.\n   Unlike other face representations, this embedding has the nice property\n   that a larger distance between two face embeddings means\n   that the faces are likely not of the same person.\n   This property makes clustering, similarity detection,\n   and classification tasks easier than other face recognition\n   techniques where the Euclidean distance between\n   features is not meaningful.\n\n\nApply your favorite clustering or classification techniques\n   to the features to complete your recognition task.\n   See below for our examples for classification and\n   similarity detection, including an online web demo.\n\n\n\n\n\n\nNews\n\n\n\n\n[Oct 15, 2015] (Spanish) GenBeta: \nOpenFace, un nuevo software de reconocimiento facial, de c\u00f3digo abierto\n\n\n[Oct 15, 2015] TheNextWeb: \nWatch this open-source program recognize faces in real time\n\n\n\n\nBlogosphere\n\n\n\n\n[Feb 24, 2016] \nHey Zuck, We Built Your Office A.I. Solution\n\n\n[Feb 3, 2016] \nRTNiFiOpenFace and WebSocketServer add face recognition to an Apache NiFi video flow\n\n\n[Jan 29, 2016] \nIntegrating OpenFace into an Apache NiFi flow using WebSockets\n\n\n\n\nProjects using OpenFace\n\n\n\n\npyannote/pyannote-video\n:\n  Face detection, tracking, and clustering in videos.\n\n\naybassiouny/OpenFaceCpp\n:\n  Unofficial C++ implementation.\n\n\n\n\nCitations\n\n\nThe following is a \nBibTeX\n\nand plaintext reference\nfor the OpenFace GitHub repository.\nThe reference may change in the future.\nThe BibTeX entry requires the \nurl\n LaTeX package.\n\n\n@misc{amos2016openface,\n    title        = {{OpenFace: Face Recognition with Deep Neural Networks}},\n    author       = {Amos, Brandon and Ludwiczuk, Bartosz and Harkes, Jan and\n                    Pillai, Padmanabhan and Elgazzar, Khalid and Satyanarayanan, Mahadev},\n    howpublished = {\\url{http://github.com/cmusatyalab/openface}},\n    note         = {Accessed: 2016-01-11}\n}\n\nBrandon Amos, Bartosz Ludwiczuk, Jan Harkes, Padmanabhan Pillai,\nKhalid Elgazzar, and Mahadev Satyanarayanan.\nOpenFace: Face Recognition with Deep Neural Networks.\nhttp://github.com/cmusatyalab/openface.\nAccessed: 2016-01-11\n\n\n\n\nAcknowledgements\n\n\n\n\nThe fantastic Torch ecosystem and community.\n\n\nAlfredo Canziani's\n\n  implementation of FaceNet's loss function in\n  \ntorch-TripletEmbedding\n.\n\n\nNicholas L\u00e9onard\n\n  for quickly merging my pull requests to\n  \nnicholas-leonard/dpnn\n\n  modifying the inception layer.\n\n\nFrancisco Massa\n\n  and\n  \nAndrej Karpathy\n\n  for\n  quickly releasing \nnn.Normalize\n\n  after I expressed interest in using it.\n\n\nSoumith Chintala\n for\n  help with the \nfbcunn\n\n  example code.\n\n\nDavis King's\n \ndlib\n\n  library for face detection and alignment.\n\n\nThe GitHub issue and pull request templates are inspired from\n  \nRandy Olsen's\n templates at \nrhiever/tpot\n,\n  \nJustin Abrahms'\n \nPR template\n,\n  and\n  \nAurelia Moser's\n \nissue template\n.\n\n\nZhuo Chen, Kiryong Ha, Wenlu Hu,\n  \nRahul Sukthankar\n, and\n  Junjue Wang for insightful discussions.\n\n\n\n\nLicensing\n\n\nUnless otherwise stated, the source code and trained Torch and Python\nmodel files are copyright Carnegie Mellon University and licensed\nunder the\n\nApache 2.0 License\n.\nPortions from the following third party sources have\nbeen modified and are included in this repository.\nThese portions are noted in the source files and are\ncopyright their respective authors with\nthe licenses listed.\n\n\n\n\n\n\n\n\nProject\n\n\nModified\n\n\nLicense\n\n\n\n\n\n\n\n\n\n\n\n\nAtcold/torch-TripletEmbedding\n\n\nNo\n\n\nMIT\n\n\n\n\n\n\n\n\nfacebook/fbnn\n\n\nYes\n\n\nBSD",
            "title": "Home"
        },
        {
            "location": "/#openface",
            "text": "Free and open source face recognition with\ndeep neural networks.",
            "title": "OpenFace "
        },
        {
            "location": "/#news",
            "text": "2016-01-19: OpenFace 0.2.0 released!\n  See  this blog post \n  for more details.    OpenFace is a Python and  Torch  implementation of\nface recognition with deep neural networks and is based on\nthe CVPR 2015 paper FaceNet: A Unified Embedding for Face Recognition and Clustering \nby Florian Schroff, Dmitry Kalenichenko, and James Philbin at Google.\nTorch allows the network to be executed on a CPU or with CUDA.  Crafted by  Brandon Amos  in Satya's  research group at\nCarnegie Mellon University.    The code is available on GitHub at\n   cmusatyalab/openface .  API Documentation  Join the\n   cmu-openface group \n  or the\n   gitter chat \n  for discussions and installation issues.  Development discussions and bugs reports are on the\n   issue tracker .    This research was supported by the National Science Foundation (NSF)\nunder grant number CNS-1518865.  Additional support\nwas provided by the Intel Corporation, Google, Vodafone, NVIDIA, and the\nConklin Kistler family fund.  Any opinions, findings, conclusions or\nrecommendations expressed in this material are those of the authors\nand should not be attributed to their employers or funding sources.   Isn't face recognition a solved problem?  No! Accuracies from research papers have just begun to surpass\nhuman accuracies on some benchmarks.\nThe accuracies of open source face recognition systems lag\nbehind the state-of-the-art.\nSee  our accuracy comparisons \non the famous LFW benchmark.   Please use responsibly!  We do not support the use of this project in applications\nthat violate privacy and security.\nWe are using this to help cognitively impaired users\nsense and understand the world around them.",
            "title": "News"
        },
        {
            "location": "/#overview",
            "text": "The following overview shows the workflow for a single input\nimage of Sylvestor Stallone from the publicly available LFW dataset .   Detect faces with a pre-trained models from\n   dlib \n  or\n   OpenCV .  Transform the face for the neural network.\n   This repository uses dlib's\n    real-time pose estimation \n   with OpenCV's\n    affine transformation \n   to try to make the eyes and bottom lip appear in\n   the same location on each image.  Use a deep neural network to represent (or embed) the face on\n   a 128-dimensional unit hypersphere.\n   The embedding is a generic representation for anybody's face.\n   Unlike other face representations, this embedding has the nice property\n   that a larger distance between two face embeddings means\n   that the faces are likely not of the same person.\n   This property makes clustering, similarity detection,\n   and classification tasks easier than other face recognition\n   techniques where the Euclidean distance between\n   features is not meaningful.  Apply your favorite clustering or classification techniques\n   to the features to complete your recognition task.\n   See below for our examples for classification and\n   similarity detection, including an online web demo.",
            "title": "Overview"
        },
        {
            "location": "/#news_1",
            "text": "[Oct 15, 2015] (Spanish) GenBeta:  OpenFace, un nuevo software de reconocimiento facial, de c\u00f3digo abierto  [Oct 15, 2015] TheNextWeb:  Watch this open-source program recognize faces in real time",
            "title": "News"
        },
        {
            "location": "/#blogosphere",
            "text": "[Feb 24, 2016]  Hey Zuck, We Built Your Office A.I. Solution  [Feb 3, 2016]  RTNiFiOpenFace and WebSocketServer add face recognition to an Apache NiFi video flow  [Jan 29, 2016]  Integrating OpenFace into an Apache NiFi flow using WebSockets",
            "title": "Blogosphere"
        },
        {
            "location": "/#projects-using-openface",
            "text": "pyannote/pyannote-video :\n  Face detection, tracking, and clustering in videos.  aybassiouny/OpenFaceCpp :\n  Unofficial C++ implementation.",
            "title": "Projects using OpenFace"
        },
        {
            "location": "/#citations",
            "text": "The following is a  BibTeX \nand plaintext reference\nfor the OpenFace GitHub repository.\nThe reference may change in the future.\nThe BibTeX entry requires the  url  LaTeX package.  @misc{amos2016openface,\n    title        = {{OpenFace: Face Recognition with Deep Neural Networks}},\n    author       = {Amos, Brandon and Ludwiczuk, Bartosz and Harkes, Jan and\n                    Pillai, Padmanabhan and Elgazzar, Khalid and Satyanarayanan, Mahadev},\n    howpublished = {\\url{http://github.com/cmusatyalab/openface}},\n    note         = {Accessed: 2016-01-11}\n}\n\nBrandon Amos, Bartosz Ludwiczuk, Jan Harkes, Padmanabhan Pillai,\nKhalid Elgazzar, and Mahadev Satyanarayanan.\nOpenFace: Face Recognition with Deep Neural Networks.\nhttp://github.com/cmusatyalab/openface.\nAccessed: 2016-01-11",
            "title": "Citations"
        },
        {
            "location": "/#acknowledgements",
            "text": "The fantastic Torch ecosystem and community.  Alfredo Canziani's \n  implementation of FaceNet's loss function in\n   torch-TripletEmbedding .  Nicholas L\u00e9onard \n  for quickly merging my pull requests to\n   nicholas-leonard/dpnn \n  modifying the inception layer.  Francisco Massa \n  and\n   Andrej Karpathy \n  for\n  quickly releasing  nn.Normalize \n  after I expressed interest in using it.  Soumith Chintala  for\n  help with the  fbcunn \n  example code.  Davis King's   dlib \n  library for face detection and alignment.  The GitHub issue and pull request templates are inspired from\n   Randy Olsen's  templates at  rhiever/tpot ,\n   Justin Abrahms'   PR template ,\n  and\n   Aurelia Moser's   issue template .  Zhuo Chen, Kiryong Ha, Wenlu Hu,\n   Rahul Sukthankar , and\n  Junjue Wang for insightful discussions.",
            "title": "Acknowledgements"
        },
        {
            "location": "/#licensing",
            "text": "Unless otherwise stated, the source code and trained Torch and Python\nmodel files are copyright Carnegie Mellon University and licensed\nunder the Apache 2.0 License .\nPortions from the following third party sources have\nbeen modified and are included in this repository.\nThese portions are noted in the source files and are\ncopyright their respective authors with\nthe licenses listed.     Project  Modified  License       Atcold/torch-TripletEmbedding  No  MIT     facebook/fbnn  Yes  BSD",
            "title": "Licensing"
        },
        {
            "location": "/demo-1-web/",
            "text": "Demo 1: Real-Time Web Demo\n\n\nSee \nour YouTube video\n\nof using this in a real-time web application\nfor face recognition.\nThe source is available in\n\ndemos/web\n.\nThe browser portions have been tested on Google Chrome 46 in OSX.\n\n\n\n\nThis demo does the full face recognition pipeline on every frame.\nIn practice, object tracking\n\nlike dlib's\n\nshould be used once the face recognizer has predicted a face.\n\n\nIn the edge case when a single person is trained,\nthe classifier has no knowledge of other people and\nlabels anybody with the name of the trained person.\n\n\nThe web demo does not predict unknown users and the saved\nfaces are only available for the browser session.\nIf you're interested in predicting unknown people,\none idea is to use a probabilistic classifier to predict\nconfidence scores and then call the prediction unknown\nif the confidence is too low.\nSee the \nclassification demo\n\nfor an example of using a probabilistic classifier.\n\n\nSee \ncowbjt's unofficial fork\n\nfor a version of the demo that saves trained faces in a SQLite\ndatabase.\n\n\n\n\nSetup and Running\n\n\nTo run on your system, first follow the\n\nSetup Guide\n and make sure you can\nrun a simpler demo, like the \ncomparison demo\n.\n\n\nIf you experience issues with the web demo,\nplease post to\n\nour mailing list\n\nand include the the WebSocket log contents from\n\n/tmp/openface.websocket.log\n if available.\n\n\nWarning when running remotely\n\n\nTrying to connect to a remote or Docker version of OpenFace in\nthe latest version of Chrome will result in the following error:\n\n\n\n\ngetUserMedia() no longer works on insecure origins. To use this\nfeature, you should consider switching your application to a secure\norigin, such as HTTPS. See https://goo.gl/rStTGz for more details.\n\n\n\n\nThey suggest three workarounds:\n\n\n\n\n\n\nlocalhost is treated as a secure origin over HTTP, so if you're\n    able to run your server from localhost, you should be able to test\n    the feature on that server.\n\n\n\n\n\n\nYou can run chrome with the\n    --unsafely-treat-insecure-origin-as-secure=\"example.com\" flag\n    (replacing \"example.com\" with the origin you actually want to test),\n    which will treat that origin as secure for this session. Note that\n    you also need to include the --user-data-dir=/test/only/profile/dir\n    to create a fresh testing profile for the flag to work.\n\n\n\n\n\n\nUse secure protocols\n\n\n\n\n\n\n\n\n#2 is requires starting Chrome with a non-standard flag.\n\n\nIf you don't want to start Chrome with a non-standard flag,\nthe following commands use \nncat\n to\nroute all OpenFace traffic through localhost to a remote server or\nDocker container so that the demo can be accessed in Chrome\nat \nhttp://localhost:8000\n.\nReplace \nSERVER_IP\n with the IP address of your server.\n\n\nexport SERVER_IP=192.168.99.100\nncat --sh-exec \"ncat $SERVER_IP 8000\" -l 8000 --keep-open &\nncat --sh-exec \"ncat $SERVER_IP 9000\" -l 9000 --keep-open &\n\n\n\n\nWe are also interested in help running this demo with secure protocols\nin \nIssue #75\n\nso the demo works on a remote server or Docker without these workarounds.\n\n\nWith Docker\n\n\nStart the HTTP and WebSocket servers on ports 8000 and 9000 in the\nDocker container with:\n\n\ndocker run -p 9000:9000 -p 8000:8000 -t -i bamos/openface /bin/bash -l -c '/root/openface/demos/web/start-servers.sh'\n\n\n\n\nThen find the IP address of the container and access the demo\nin your browser at \nhttp://docker-ip:8000\n.\n\n\nManual Setup\n\n\nAfter following the OpenFace setup guide and successfully running the\ncomparison demo, install the requirements for the web demo with\n\nsudo pip install -r requirements.txt\n\nfrom the \ndemos/web\n directory.\n\n\nStart the HTTP and WebSocket servers on ports 8000 and 9000, respectively,\nwith \n./demos/web/start-servers.sh\n.\nIf you wish to use other ports,\npass them as \n./demos/web/start-servers.sh HTTP_PORT WEBSOCKET_PORT\n.\n\n\nYou should now also be able to access the demo from your browser\nat \nhttp://localhost:8000\n if running locally or\n\nhttp://your-server:8000\n if running on a server.",
            "title": "Demo 1 - Real-time Web"
        },
        {
            "location": "/demo-1-web/#demo-1-real-time-web-demo",
            "text": "See  our YouTube video \nof using this in a real-time web application\nfor face recognition.\nThe source is available in demos/web .\nThe browser portions have been tested on Google Chrome 46 in OSX.   This demo does the full face recognition pipeline on every frame.\nIn practice, object tracking like dlib's \nshould be used once the face recognizer has predicted a face.  In the edge case when a single person is trained,\nthe classifier has no knowledge of other people and\nlabels anybody with the name of the trained person.  The web demo does not predict unknown users and the saved\nfaces are only available for the browser session.\nIf you're interested in predicting unknown people,\none idea is to use a probabilistic classifier to predict\nconfidence scores and then call the prediction unknown\nif the confidence is too low.\nSee the  classification demo \nfor an example of using a probabilistic classifier.  See  cowbjt's unofficial fork \nfor a version of the demo that saves trained faces in a SQLite\ndatabase.",
            "title": "Demo 1: Real-Time Web Demo"
        },
        {
            "location": "/demo-1-web/#setup-and-running",
            "text": "To run on your system, first follow the Setup Guide  and make sure you can\nrun a simpler demo, like the  comparison demo .  If you experience issues with the web demo,\nplease post to our mailing list \nand include the the WebSocket log contents from /tmp/openface.websocket.log  if available.  Warning when running remotely  Trying to connect to a remote or Docker version of OpenFace in\nthe latest version of Chrome will result in the following error:   getUserMedia() no longer works on insecure origins. To use this\nfeature, you should consider switching your application to a secure\norigin, such as HTTPS. See https://goo.gl/rStTGz for more details.   They suggest three workarounds:    localhost is treated as a secure origin over HTTP, so if you're\n    able to run your server from localhost, you should be able to test\n    the feature on that server.    You can run chrome with the\n    --unsafely-treat-insecure-origin-as-secure=\"example.com\" flag\n    (replacing \"example.com\" with the origin you actually want to test),\n    which will treat that origin as secure for this session. Note that\n    you also need to include the --user-data-dir=/test/only/profile/dir\n    to create a fresh testing profile for the flag to work.    Use secure protocols     #2 is requires starting Chrome with a non-standard flag.  If you don't want to start Chrome with a non-standard flag,\nthe following commands use  ncat  to\nroute all OpenFace traffic through localhost to a remote server or\nDocker container so that the demo can be accessed in Chrome\nat  http://localhost:8000 .\nReplace  SERVER_IP  with the IP address of your server.  export SERVER_IP=192.168.99.100\nncat --sh-exec \"ncat $SERVER_IP 8000\" -l 8000 --keep-open &\nncat --sh-exec \"ncat $SERVER_IP 9000\" -l 9000 --keep-open &  We are also interested in help running this demo with secure protocols\nin  Issue #75 \nso the demo works on a remote server or Docker without these workarounds.  With Docker  Start the HTTP and WebSocket servers on ports 8000 and 9000 in the\nDocker container with:  docker run -p 9000:9000 -p 8000:8000 -t -i bamos/openface /bin/bash -l -c '/root/openface/demos/web/start-servers.sh'  Then find the IP address of the container and access the demo\nin your browser at  http://docker-ip:8000 .  Manual Setup  After following the OpenFace setup guide and successfully running the\ncomparison demo, install the requirements for the web demo with sudo pip install -r requirements.txt \nfrom the  demos/web  directory.  Start the HTTP and WebSocket servers on ports 8000 and 9000, respectively,\nwith  ./demos/web/start-servers.sh .\nIf you wish to use other ports,\npass them as  ./demos/web/start-servers.sh HTTP_PORT WEBSOCKET_PORT .  You should now also be able to access the demo from your browser\nat  http://localhost:8000  if running locally or http://your-server:8000  if running on a server.",
            "title": "Setup and Running"
        },
        {
            "location": "/demo-2-comparison/",
            "text": "Demo 2: Comparing two images\n\n\nThe \ncomparison demo\n outputs the predicted similarity\nscore of two faces by computing the squared L2 distance between\ntheir representations.\nA lower score indicates two faces are more likely of the same person.\nSince the representations are on the unit hypersphere, the\nscores range from 0 (the same picture) to 4.0.\nThe following distances between images of John Lennon and\nEric Clapton were generated with\n\n./demos/compare.py images/examples/{lennon*,clapton*}\n.\n\n\n\n\n\n\n\n\nLennon 1\n\n\nLennon 2\n\n\nClapton 1\n\n\nClapton 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following table shows that a distance threshold of \n0.99\n would\ndistinguish these two people.\nIn practice, further experimentation should be done on the distance threshold.\nOn our LFW experiments, the mean threshold across multiple\nexperiments is \n0.99\n,\nsee \naccuracies.txt\n.\n\n\n\n\n\n\n\n\nImage 1\n\n\nImage 2\n\n\nDistance\n\n\n\n\n\n\n\n\n\n\nLennon 1\n\n\nLennon 2\n\n\n0.763\n\n\n\n\n\n\nLennon 1\n\n\nClapton 1\n\n\n1.132\n\n\n\n\n\n\nLennon 1\n\n\nClapton 2\n\n\n1.145\n\n\n\n\n\n\nLennon 2\n\n\nClapton 1\n\n\n1.447\n\n\n\n\n\n\nLennon 2\n\n\nClapton 2\n\n\n1.521\n\n\n\n\n\n\nClapton 1\n\n\nClapton 2\n\n\n0.318",
            "title": "Demo 2 - Comparison"
        },
        {
            "location": "/demo-2-comparison/#demo-2-comparing-two-images",
            "text": "The  comparison demo  outputs the predicted similarity\nscore of two faces by computing the squared L2 distance between\ntheir representations.\nA lower score indicates two faces are more likely of the same person.\nSince the representations are on the unit hypersphere, the\nscores range from 0 (the same picture) to 4.0.\nThe following distances between images of John Lennon and\nEric Clapton were generated with ./demos/compare.py images/examples/{lennon*,clapton*} .     Lennon 1  Lennon 2  Clapton 1  Clapton 2             The following table shows that a distance threshold of  0.99  would\ndistinguish these two people.\nIn practice, further experimentation should be done on the distance threshold.\nOn our LFW experiments, the mean threshold across multiple\nexperiments is  0.99 ,\nsee  accuracies.txt .     Image 1  Image 2  Distance      Lennon 1  Lennon 2  0.763    Lennon 1  Clapton 1  1.132    Lennon 1  Clapton 2  1.145    Lennon 2  Clapton 1  1.447    Lennon 2  Clapton 2  1.521    Clapton 1  Clapton 2  0.318",
            "title": "Demo 2: Comparing two images"
        },
        {
            "location": "/demo-3-classifier/",
            "text": "Demo 3: Training a Classifier\n\n\nOpenFace's core provides a feature extraction method to\nobtain a low-dimensional representation of any face.\n\ndemos/classifier.py\n\nshows a demo of how these representations can be\nused to create a face classifier.\n\n\nThere is a distinction between training the deep neural network (DNN)\nmodel for feature representation\nand training a model for classifying people with the DNN model.\nThis shows how to use a pre-trained DNN model to train and use\na classification model.\n\n\nCreating a Classification Model\n\n\n1. Create raw image directory.\n\n\nCreate a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as \njpg\n or \npng\n and have\na lowercase extension.\n\n\n$ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png\n\n\n\n\n2. Preprocess the raw images\n\n\nChange \n8\n to however many\nseparate processes you want to run:\n\nfor N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align outerEyesAndNose <path-to-aligned-data> --size 96 & done\n.\n\n\nIf failed alignment attempts causes your directory to have too few images,\nyou can use our utility script\n\n./util/prune-dataset.py\n\nto deletes directories with less than a specified number of images.\n\n\n3. Generate Representations\n\n\n./batch-represent/main.lua -outDir <feature-directory> -data <path-to-aligned-data>\n\ncreates \nreps.csv\n and \nlabels.csv\n in \n<feature-directory>\n.\n\n\n4. Create the Classification Model\n\n\nUse \n./demos/classifier.py train <feature-directory>\n to produce\nthe classification model which is an SVM saved to disk as\na Python pickle.\n\n\nTraining uses \nscikit-learn\n to perform\na grid search over SVM parameters.\nFor 1000's of images, training the SVMs takes seconds.\n\n\nClassifying New Images\n\n\nWe have released a \nceleb-classifier.nn4.small2.v1.pkl\n classification model\nthat is trained on about 6000 total images of the following people,\nwhich are the people with the most images in our dataset.\nClassifiers can be created with far less images per\nperson.\n\n\n\n\nAmerica Ferrera\n\n\nAmy Adams\n\n\nAnne Hathaway\n\n\nBen Stiller\n\n\nBradley Cooper\n\n\nDavid Boreanaz\n\n\nEmily Deschanel\n\n\nEva Longoria\n\n\nJon Hamm\n\n\nSteve Carell\n\n\n\n\nFor an example, consider the following small set of images\nthe model has no knowledge of.\nFor an unknown person, a prediction still needs to be made, but\nthe confidence score is usually lower.\n\n\nRun the classifier with:\n\n\n./demos/classifier.py infer ./models/openface/celeb-classifier.nn4.small2.v1.pkl images/examples/{carell,adams,lennon}*\n\n\n\n\n\n\n\n\n\n\nPerson\n\n\nImage\n\n\nPrediction\n\n\nConfidence\n\n\n\n\n\n\n\n\n\n\nCarell\n\n\n\n\nSteveCarell\n\n\n0.97\n\n\n\n\n\n\nAdams\n\n\n\n\nAmyAdams\n\n\n0.81\n\n\n\n\n\n\nLennon 1 (Unknown)\n\n\n\n\nSteveCarell\n\n\n0.50\n\n\n\n\n\n\nLennon 2 (Unknown)\n\n\n\n\nDavidBoreanaz\n\n\n0.43\n\n\n\n\n\n\n\n\nMinimal Working Example to Extract Features\n\n\nmkdir -p classify-test/raw/{lennon,clapton}\ncp images/examples/lennon-* classify-test/raw/lennon\ncp images/examples/clapton-* classify-test/raw/clapton\n./util/align-dlib.py classify-test/raw align outerEyesAndNose classify-test/aligned --size 96\n./batch-represent/main.lua -outDir classify-test/features -data classify-test/aligned\n...\nnImgs: \u00a04\nRepresent: 4/4",
            "title": "Demo 3 - Training a Classifier"
        },
        {
            "location": "/demo-3-classifier/#demo-3-training-a-classifier",
            "text": "OpenFace's core provides a feature extraction method to\nobtain a low-dimensional representation of any face. demos/classifier.py \nshows a demo of how these representations can be\nused to create a face classifier.  There is a distinction between training the deep neural network (DNN)\nmodel for feature representation\nand training a model for classifying people with the DNN model.\nThis shows how to use a pre-trained DNN model to train and use\na classification model.",
            "title": "Demo 3: Training a Classifier"
        },
        {
            "location": "/demo-3-classifier/#creating-a-classification-model",
            "text": "1. Create raw image directory.  Create a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as  jpg  or  png  and have\na lowercase extension.  $ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png  2. Preprocess the raw images  Change  8  to however many\nseparate processes you want to run: for N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align outerEyesAndNose <path-to-aligned-data> --size 96 & done .  If failed alignment attempts causes your directory to have too few images,\nyou can use our utility script ./util/prune-dataset.py \nto deletes directories with less than a specified number of images.  3. Generate Representations  ./batch-represent/main.lua -outDir <feature-directory> -data <path-to-aligned-data> \ncreates  reps.csv  and  labels.csv  in  <feature-directory> .  4. Create the Classification Model  Use  ./demos/classifier.py train <feature-directory>  to produce\nthe classification model which is an SVM saved to disk as\na Python pickle.  Training uses  scikit-learn  to perform\na grid search over SVM parameters.\nFor 1000's of images, training the SVMs takes seconds.",
            "title": "Creating a Classification Model"
        },
        {
            "location": "/demo-3-classifier/#classifying-new-images",
            "text": "We have released a  celeb-classifier.nn4.small2.v1.pkl  classification model\nthat is trained on about 6000 total images of the following people,\nwhich are the people with the most images in our dataset.\nClassifiers can be created with far less images per\nperson.   America Ferrera  Amy Adams  Anne Hathaway  Ben Stiller  Bradley Cooper  David Boreanaz  Emily Deschanel  Eva Longoria  Jon Hamm  Steve Carell   For an example, consider the following small set of images\nthe model has no knowledge of.\nFor an unknown person, a prediction still needs to be made, but\nthe confidence score is usually lower.  Run the classifier with:  ./demos/classifier.py infer ./models/openface/celeb-classifier.nn4.small2.v1.pkl images/examples/{carell,adams,lennon}*     Person  Image  Prediction  Confidence      Carell   SteveCarell  0.97    Adams   AmyAdams  0.81    Lennon 1 (Unknown)   SteveCarell  0.50    Lennon 2 (Unknown)   DavidBoreanaz  0.43",
            "title": "Classifying New Images"
        },
        {
            "location": "/demo-3-classifier/#minimal-working-example-to-extract-features",
            "text": "mkdir -p classify-test/raw/{lennon,clapton}\ncp images/examples/lennon-* classify-test/raw/lennon\ncp images/examples/clapton-* classify-test/raw/clapton\n./util/align-dlib.py classify-test/raw align outerEyesAndNose classify-test/aligned --size 96\n./batch-represent/main.lua -outDir classify-test/features -data classify-test/aligned\n...\nnImgs: \u00a04\nRepresent: 4/4",
            "title": "Minimal Working Example to Extract Features"
        },
        {
            "location": "/usage/",
            "text": "Usage\n\n\nAPI Documentation\n\n\nExample\n\n\nSee \nthe image comparison demo\n for a complete example\nwritten in Python using a naive Torch subprocess to process the faces.\n\n\nimport openface\n\n# `args` are parsed command-line arguments.\n\nalign = openface.AlignDlib(args.dlibFacePredictor)\nnet = openface.TorchNeuralNet(args.networkModel, args.imgDim, cuda=args.cuda)\n\n# `img` is a numpy matrix containing the RGB pixels of the image.\nbb = align.getLargestFaceBoundingBox(img)\nalignedFace = align.align(args.imgDim, img, bb,\n                          landmarkIndices=openface.AlignDlib.OUTER_EYES_AND_NOSE)\nrep1 = net.forward(alignedFace)\n\n# `rep2` obtained similarly.\nd = rep1 - rep2\ndistance = np.dot(d, d)",
            "title": "Usage and API Docs"
        },
        {
            "location": "/usage/#usage",
            "text": "",
            "title": "Usage"
        },
        {
            "location": "/usage/#api-documentation",
            "text": "",
            "title": "API Documentation"
        },
        {
            "location": "/usage/#example",
            "text": "See  the image comparison demo  for a complete example\nwritten in Python using a naive Torch subprocess to process the faces.  import openface\n\n# `args` are parsed command-line arguments.\n\nalign = openface.AlignDlib(args.dlibFacePredictor)\nnet = openface.TorchNeuralNet(args.networkModel, args.imgDim, cuda=args.cuda)\n\n# `img` is a numpy matrix containing the RGB pixels of the image.\nbb = align.getLargestFaceBoundingBox(img)\nalignedFace = align.align(args.imgDim, img, bb,\n                          landmarkIndices=openface.AlignDlib.OUTER_EYES_AND_NOSE)\nrep1 = net.forward(alignedFace)\n\n# `rep2` obtained similarly.\nd = rep1 - rep2\ndistance = np.dot(d, d)",
            "title": "Example"
        },
        {
            "location": "/setup/",
            "text": "Setup\n\n\nThe following instructions are for Linux and OSX only.\nPlease contribute modifications and build instructions if you\nare interested in running this on other operating systems.\n\n\n\n\nWe strongly recommend using the \nDocker\n\n  container unless you are experienced with building\n  Linux software from source.\n\n\nIn OSX, you may have to change the hashbangs\n  from \npython2\n to \npython\n.\n\n\nOpenFace has been tested in Ubuntu 14.04 and OSX 10.10\n  and may not work well on other distributions.\n  Please let us know of any challenges you had to overcome\n  getting OpenFace to work on other distributions.\n\n\n\n\nWarning for architectures other than 64-bit x86\n\n\nSee \n#42\n.\n\n\nCheck out git submodules\n\n\nClone with \n--recursive\n or run \ngit submodule init && git submodule update\n\nafter checking out.\n\n\nWith Docker\n\n\nThis repo can be used as a container with\n\nDocker\n for CPU mode.\nDepending on your Docker configuration, you may need to\nrun the docker commands as root.\n\n\nAutomated Docker Build\n\n\nThe quickest way to getting started is to use our pre-built\nautomated Docker build, which is available from\n\nbamos/openface\n.\nThis does not require or use a locally checked out copy of OpenFace.\nTo use on your images, share a directory between your\nhost and the Docker container.\n\n\ndocker pull bamos/openface\ndocker run -p 9000:9000 -p 8000:8000 -t -i bamos/openface /bin/bash\ncd /root/openface\n./demos/compare.py images/examples/{lennon*,clapton*}\n./demos/classifier.py infer models/openface/celeb-classifier.nn4.small2.v1.pkl ./images/examples/carell.jpg\n./demos/web/start-servers.sh\n\n\n\n\nBuilding a Docker Container\n\n\nThis builds a Docker container from a locally checked out copy of OpenFace,\nwhich will take about 2 hours on a modern machine.\nBe sure you have checked out the git submodules.\nRun the following commands from the \nopenface\n directory.\n\n\ndocker build -t openface .\ndocker run -p 9000:9000 -p 8000:8000 -t -i openface /bin/bash\ncd /root/openface\n./run-tests.sh\n./demos/compare.py images/examples/{lennon*,clapton*}\n./demos/classifier.py infer models/openface/celeb-classifier.nn4.small2.v1.pkl ./images/examples/carell.jpg\n./demos/web/start-servers.sh\n\n\n\n\nDocker in OSX\n\n\nIn OSX, follow the\n\nDocker Mac OSX Installation Guide\n\nand start a docker machine and connect your shell to it\nbefore trying to build the container.\nIn the simplest case, this can be done with:\n\n\ndocker-machine create --driver virtualbox --virtualbox-memory 4096 default\neval $(docker-machine env default)\n\n\n\n\nDocker memory issues in OSX\n\n\nSome users have reported the following silent Torch/Lua failure\nwhen running \nbatch-represent\n caused by an out of memory issue.\n\n\n/root/torch/install/bin/luajit: /openface/batch-represent/dataset.lua:191: attempt to perform arithmetic on a nil value\n\n\n\n\nIf you're experiencing this, make sure you have created a Docker machine\nwith at least 4GB of memory with \n--virtualbox-memory 4096\n.\n\n\nBy hand\n\n\nBe sure you have checked out the submodules and downloaded the models as\ndescribed above.\nSee the\n\nDockerfile\n\nas a reference.\n\n\nThis project uses \npython2\n because of the \nopencv\n\nand \ndlib\n dependencies.\nInstall the packages the Dockerfile uses with your package manager.\nWith \npip2\n, install \nnumpy\n, \npandas\n, \nscipy\n, \nscikit-learn\n, and \nscikit-image\n.\n\n\nNext, manually install the following.\n\n\nOpenCV\n\n\nDownload \nOpenCV 2.4.11\n\nand follow their\n\nbuild instructions\n.\n\n\ndlib\n\n\ndlib can be installed from \npypi\n\nor built manually and depends on boost libraries.\nBuilding dlib manually with\n\nAVX support\n\nprovides higher performance.\n\n\nTo build manually, download\n\ndlib v18.16\n,\nthen run the following commands.\nFor the final command, make sure the directory is in your default\nPython path, which can be found with \nsys.path\n in a Python interpreter.\nIn OSX, use \nsite-packages\n instead of \ndist-packages\n.\n\n\nmkdir -p ~/src\ncd ~/src\ntar xf dlib-18.16.tar.bz2\ncd dlib-18.16/python_examples\nmkdir build\ncd build\ncmake ../../tools/python\ncmake --build . --config Release\nsudo cp dlib.so /usr/local/lib/python2.7/dist-packages\n\n\n\n\nAt this point, you should be able to start your \npython2\n\ninterpreter and successfully run \nimport cv2; import dlib\n.\n\n\nIn OSX, you may get a \nFatal Python error: PyThreadState_Get: no current thread\n.\nYou may be able to resolve by rebuilding \npython\n and \nboost-python\n\nas reported in \n#21\n,\nbut please file a new issue with us or \ndlib\n\nif you are unable to resolve this.\n\n\nTorch\n\n\nInstall \nTorch\n from the instructions on their website.\nAt this point, the command-line program \nth\n should\nbe available in your shell.\nInstall the dependencies with \nluarocks install $NAME\n,\nwhere \n$NAME\n is as listed below.\n\n\n\n\ndpnn\n\n\nnn\n\n\noptim\n\n\ncsvigo\n\n\ncutorch\n and \ncunn\n\n  (only with CUDA)\n\n\nfblualib\n\n  (only for \ntraining a DNN\n)\n\n\ntorchx\n\n  (only for \ntraining a DNN\n)\n\n\noptnet\n\n  (optional, only for \ntraining a DNN\n)\n\n\n\n\nThese can all be installed with:\n\n\nfor NAME in dpnn nn optim optnet csvigo cutorch cunn fblualib torchx; do luarocks install $NAME; done\n\n\n\n\nOpenFace\n\n\nIn OSX, install \nfindutils\n and \ncoreutils\n with Brew or MacPorts for\nthe prefixed GNU variants \ngfind\n and \ngwc\n.\nThese are required for the commands to be compatible with\nthe Linux defaults of these commands.\n\n\nFrom the root OpenFace directory,\ninstall the Python dependencies with\n\nsudo python2 setup.py install\n.\n\n\nRun \nmodels/get-models.sh\n\nto download pre-trained OpenFace\nmodels on the combined CASIA-WebFace and FaceScrub database.\nThis also downloads dlib's pre-trained model for face landmark detection.\nThis will incur about 200MB of network traffic.",
            "title": "Setup"
        },
        {
            "location": "/setup/#setup",
            "text": "The following instructions are for Linux and OSX only.\nPlease contribute modifications and build instructions if you\nare interested in running this on other operating systems.   We strongly recommend using the  Docker \n  container unless you are experienced with building\n  Linux software from source.  In OSX, you may have to change the hashbangs\n  from  python2  to  python .  OpenFace has been tested in Ubuntu 14.04 and OSX 10.10\n  and may not work well on other distributions.\n  Please let us know of any challenges you had to overcome\n  getting OpenFace to work on other distributions.",
            "title": "Setup"
        },
        {
            "location": "/setup/#warning-for-architectures-other-than-64-bit-x86",
            "text": "See  #42 .",
            "title": "Warning for architectures other than 64-bit x86"
        },
        {
            "location": "/setup/#check-out-git-submodules",
            "text": "Clone with  --recursive  or run  git submodule init && git submodule update \nafter checking out.",
            "title": "Check out git submodules"
        },
        {
            "location": "/setup/#with-docker",
            "text": "This repo can be used as a container with Docker  for CPU mode.\nDepending on your Docker configuration, you may need to\nrun the docker commands as root.  Automated Docker Build  The quickest way to getting started is to use our pre-built\nautomated Docker build, which is available from bamos/openface .\nThis does not require or use a locally checked out copy of OpenFace.\nTo use on your images, share a directory between your\nhost and the Docker container.  docker pull bamos/openface\ndocker run -p 9000:9000 -p 8000:8000 -t -i bamos/openface /bin/bash\ncd /root/openface\n./demos/compare.py images/examples/{lennon*,clapton*}\n./demos/classifier.py infer models/openface/celeb-classifier.nn4.small2.v1.pkl ./images/examples/carell.jpg\n./demos/web/start-servers.sh  Building a Docker Container  This builds a Docker container from a locally checked out copy of OpenFace,\nwhich will take about 2 hours on a modern machine.\nBe sure you have checked out the git submodules.\nRun the following commands from the  openface  directory.  docker build -t openface .\ndocker run -p 9000:9000 -p 8000:8000 -t -i openface /bin/bash\ncd /root/openface\n./run-tests.sh\n./demos/compare.py images/examples/{lennon*,clapton*}\n./demos/classifier.py infer models/openface/celeb-classifier.nn4.small2.v1.pkl ./images/examples/carell.jpg\n./demos/web/start-servers.sh  Docker in OSX  In OSX, follow the Docker Mac OSX Installation Guide \nand start a docker machine and connect your shell to it\nbefore trying to build the container.\nIn the simplest case, this can be done with:  docker-machine create --driver virtualbox --virtualbox-memory 4096 default\neval $(docker-machine env default)  Docker memory issues in OSX  Some users have reported the following silent Torch/Lua failure\nwhen running  batch-represent  caused by an out of memory issue.  /root/torch/install/bin/luajit: /openface/batch-represent/dataset.lua:191: attempt to perform arithmetic on a nil value  If you're experiencing this, make sure you have created a Docker machine\nwith at least 4GB of memory with  --virtualbox-memory 4096 .",
            "title": "With Docker"
        },
        {
            "location": "/setup/#by-hand",
            "text": "Be sure you have checked out the submodules and downloaded the models as\ndescribed above.\nSee the Dockerfile \nas a reference.  This project uses  python2  because of the  opencv \nand  dlib  dependencies.\nInstall the packages the Dockerfile uses with your package manager.\nWith  pip2 , install  numpy ,  pandas ,  scipy ,  scikit-learn , and  scikit-image .  Next, manually install the following.  OpenCV  Download  OpenCV 2.4.11 \nand follow their build instructions .  dlib  dlib can be installed from  pypi \nor built manually and depends on boost libraries.\nBuilding dlib manually with AVX support \nprovides higher performance.  To build manually, download dlib v18.16 ,\nthen run the following commands.\nFor the final command, make sure the directory is in your default\nPython path, which can be found with  sys.path  in a Python interpreter.\nIn OSX, use  site-packages  instead of  dist-packages .  mkdir -p ~/src\ncd ~/src\ntar xf dlib-18.16.tar.bz2\ncd dlib-18.16/python_examples\nmkdir build\ncd build\ncmake ../../tools/python\ncmake --build . --config Release\nsudo cp dlib.so /usr/local/lib/python2.7/dist-packages  At this point, you should be able to start your  python2 \ninterpreter and successfully run  import cv2; import dlib .  In OSX, you may get a  Fatal Python error: PyThreadState_Get: no current thread .\nYou may be able to resolve by rebuilding  python  and  boost-python \nas reported in  #21 ,\nbut please file a new issue with us or  dlib \nif you are unable to resolve this.  Torch  Install  Torch  from the instructions on their website.\nAt this point, the command-line program  th  should\nbe available in your shell.\nInstall the dependencies with  luarocks install $NAME ,\nwhere  $NAME  is as listed below.   dpnn  nn  optim  csvigo  cutorch  and  cunn \n  (only with CUDA)  fblualib \n  (only for  training a DNN )  torchx \n  (only for  training a DNN )  optnet \n  (optional, only for  training a DNN )   These can all be installed with:  for NAME in dpnn nn optim optnet csvigo cutorch cunn fblualib torchx; do luarocks install $NAME; done  OpenFace  In OSX, install  findutils  and  coreutils  with Brew or MacPorts for\nthe prefixed GNU variants  gfind  and  gwc .\nThese are required for the commands to be compatible with\nthe Linux defaults of these commands.  From the root OpenFace directory,\ninstall the Python dependencies with sudo python2 setup.py install .  Run  models/get-models.sh \nto download pre-trained OpenFace\nmodels on the combined CASIA-WebFace and FaceScrub database.\nThis also downloads dlib's pre-trained model for face landmark detection.\nThis will incur about 200MB of network traffic.",
            "title": "By hand"
        },
        {
            "location": "/faq/",
            "text": "FAQ\n\n\nHow much time does OpenFace take to process an image?\n\n\nThe execution time depends on the size of the input images.\nThe following results are from processing these example images\nof John Lennon and Steve Carell, which are respectively sized\n1050x1400px and 891x601px on an 8 core 3.70 GHz CPU.\nThe network processing time is significantly less on a GPU.\n\n\n\n\n\n\nMore time is spent using the off-the-shelf face detector\nthan in the deep neural network!\nThe alignment cost is negligible.\nThese times are obtained from averaging 100 trials with\nour \nutil/profile-pipeline.py\n\nscript.\n\n\n\n\n\n\n\nHow can I make OpenFace run faster?\n\n\n\n\n\n\nResize your images so that faces are approximately 100x100 pixels\n  before running detection and alignment.\n\n\n\n\n\n\nCompile dlib with AVX instructions, as discussed\n  \nhere\n.\n  Use the \n-DUSE_AVX_INSTRUCTIONS=ON\n in the first \ncmake\n command.\n  If your architecture does not support AVX, try SSE4 or SSE2.\n\n\n\n\n\n\nMake sure Torch is linking with \nOpenBLAS\n,\n   instead of netlib for BLAS and LAPACK.\n   From our experiments, a single neural network forward pass that\n   executes in 460ms with netlib executes in 59ms with OpenBLAS.\n\n\n\n\n\n\nI'm getting an illegal instruction error in the pre-built Docker container.\n\n\nThis is unfortunately a result of building the Docker container\non one machine that compiles software with non-standard CPU flags\nand creates illegal instructions on architectures that don't support\nthe additional CPU features.\nUsing the binaries from the pre-built container on a CPU that\ndoesn't support these features results in the illegal instruction error.\nWe try to prevent these as much as possible by building the images\ninside of a Docker machine.\nIf you are still having these issues, please fall back to building\nthe image from scratch instead of pulling from Docker Hub.\nYou'll need to build the\n\nopencv-dlib-torch Dockerfile\n,\nchange the \nFROM\n part of the\n\nOpenFace Dockerfile\n\nto your version,\nthen build the OpenFace Dockerfile.",
            "title": "FAQ"
        },
        {
            "location": "/faq/#faq",
            "text": "",
            "title": "FAQ"
        },
        {
            "location": "/faq/#how-much-time-does-openface-take-to-process-an-image",
            "text": "The execution time depends on the size of the input images.\nThe following results are from processing these example images\nof John Lennon and Steve Carell, which are respectively sized\n1050x1400px and 891x601px on an 8 core 3.70 GHz CPU.\nThe network processing time is significantly less on a GPU.    More time is spent using the off-the-shelf face detector\nthan in the deep neural network!\nThe alignment cost is negligible.\nThese times are obtained from averaging 100 trials with\nour  util/profile-pipeline.py \nscript.",
            "title": "How much time does OpenFace take to process an image?"
        },
        {
            "location": "/faq/#how-can-i-make-openface-run-faster",
            "text": "Resize your images so that faces are approximately 100x100 pixels\n  before running detection and alignment.    Compile dlib with AVX instructions, as discussed\n   here .\n  Use the  -DUSE_AVX_INSTRUCTIONS=ON  in the first  cmake  command.\n  If your architecture does not support AVX, try SSE4 or SSE2.    Make sure Torch is linking with  OpenBLAS ,\n   instead of netlib for BLAS and LAPACK.\n   From our experiments, a single neural network forward pass that\n   executes in 460ms with netlib executes in 59ms with OpenBLAS.",
            "title": "How can I make OpenFace run faster?"
        },
        {
            "location": "/faq/#im-getting-an-illegal-instruction-error-in-the-pre-built-docker-container",
            "text": "This is unfortunately a result of building the Docker container\non one machine that compiles software with non-standard CPU flags\nand creates illegal instructions on architectures that don't support\nthe additional CPU features.\nUsing the binaries from the pre-built container on a CPU that\ndoesn't support these features results in the illegal instruction error.\nWe try to prevent these as much as possible by building the images\ninside of a Docker machine.\nIf you are still having these issues, please fall back to building\nthe image from scratch instead of pulling from Docker Hub.\nYou'll need to build the opencv-dlib-torch Dockerfile ,\nchange the  FROM  part of the OpenFace Dockerfile \nto your version,\nthen build the OpenFace Dockerfile.",
            "title": "I'm getting an illegal instruction error in the pre-built Docker container."
        },
        {
            "location": "/models-and-accuracies/",
            "text": "Models and Accuracies\n\n\nThis page overviews different OpenFace neural network models\nand is intended for advanced users.\n\n\nModel Definitions\n\n\nThe number of parameters are with 128-dimensional embeddings\nand do not include the batch normalization running means and\nvariances.\n\n\n\n\n\n\n\n\nModel\n\n\nNumber of Parameters\n\n\n\n\n\n\n\n\n\n\nnn4.small2\n\n\n3733968\n\n\n\n\n\n\nnn4.small1\n\n\n5579520\n\n\n\n\n\n\nnn4\n\n\n6959088\n\n\n\n\n\n\nnn2\n\n\n7472144\n\n\n\n\n\n\n\n\nPre-trained Models\n\n\nModels can be trained in different ways with different datasets.\nPre-trained models are versioned and should be released with\na corresponding model definition.\nSwitch between models with caution because the embeddings\nnot compatible with each other.\n\n\nThe current models are trained with a combination of the two largest\n(of August 2015) publicly-available face recognition datasets based on names:\n\nFaceScrub\n\nand \nCASIA-WebFace\n.\n\n\nThe models can be downloaded from our storage servers:\n\n\n\n\nnn4.v1\n\n\nnn4.v2\n\n\nnn4.small1.v1\n\n\nnn4.small2.v1\n\n\n\n\nAPI differences between the models are:\n\n\n\n\n\n\n\n\nModel\n\n\nalignment \nlandmarkIndices\n\n\n\n\n\n\n\n\n\n\nnn4.v1\n\n\nopenface.AlignDlib.INNER_EYES_AND_BOTTOM_LIP\n\n\n\n\n\n\nnn4.v2\n\n\nopenface.AlignDlib.OUTER_EYES_AND_NOSE\n\n\n\n\n\n\nnn4.small1.v1\n\n\nopenface.AlignDlib.OUTER_EYES_AND_NOSE\n\n\n\n\n\n\nnn4.small2.v1\n\n\nopenface.AlignDlib.OUTER_EYES_AND_NOSE\n\n\n\n\n\n\n\n\nPerformance\n\n\nThe performance is measured by averaging 500 forward passes with\n\nutil/profile-network.lua\n\nand the following results use OpenBLAS on an 8 core 3.70 GHz CPU\nand a Tesla K40 GPU.\n\n\n\n\n\n\n\n\nModel\n\n\nRuntime (CPU)\n\n\nRuntime (GPU)\n\n\n\n\n\n\n\n\n\n\nnn4.v1\n\n\n75.67 ms \u00b1 19.97 ms\n\n\n21.96 ms \u00b1 6.71 ms\n\n\n\n\n\n\nnn4.v2\n\n\n82.74 ms \u00b1 19.96 ms\n\n\n20.82 ms \u00b1 6.03 ms\n\n\n\n\n\n\nnn4.small1.v1\n\n\n69.58 ms \u00b1 16.17 ms\n\n\n15.90 ms \u00b1 5.18 ms\n\n\n\n\n\n\nnn4.small2.v1\n\n\n58.9 ms \u00b1 15.36 ms\n\n\n13.72 ms \u00b1 4.64 ms\n\n\n\n\n\n\n\n\nAccuracy on the LFW Benchmark\n\n\nEven though the public datasets we trained on have orders of magnitude less data\nthan private industry datasets, the accuracy is remarkably high\non the standard\n\nLFW\n\nbenchmark.\nWe had to fallback to using the deep funneled versions for\n58 of 13233 images because dlib failed to detect a face or landmarks.\n\n\n\n\n\n\n\n\nModel\n\n\nAccuracy\n\n\nAUC\n\n\n\n\n\n\n\n\n\n\nnn4.small2.v1\n (Default)\n\n\n0.9292 \u00b1 0.0134\n\n\n0.973\n\n\n\n\n\n\nnn4.small1.v1\n\n\n0.9210 \u00b1 0.0160\n\n\n0.973\n\n\n\n\n\n\nnn4.v2\n\n\n0.9157 \u00b1 0.0152\n\n\n0.966\n\n\n\n\n\n\nnn4.v1\n\n\n0.7612 \u00b1 0.0189\n\n\n0.853\n\n\n\n\n\n\nFaceNet Paper (Reference)\n\n\n0.9963 \u00b1 0.009\n\n\nnot provided\n\n\n\n\n\n\n\n\nROC Curves\n\n\nnn4.small2.v1\n\n\n\n\nnn4.small1.v1\n\n\n\n\nnn4.v2\n\n\n\n\nnn4.v1\n\n\n\n\nRunning The LFW Experiment\n\n\nThis can be generated with the following commands from the root \nopenface\n\ndirectory, assuming you have downloaded and placed the raw and\n\ndeep funneled\n\nLFW data from \nhere\n\nin \n./data/lfw/raw\n and \n./data/lfw/deepfunneled\n.\nAlso save \npairs.txt\n in\n\n./data/lfw/pairs.txt\n.\n\n\n\n\nInstall prerequisites as below.\n\n\nPreprocess the raw \nlfw\n images, change \n8\n to however many\n   separate processes you want to run:\n   \nfor N in {1..8}; do ./util/align-dlib.py data/lfw/raw align outerEyesAndNose data/lfw/dlib-affine-sz:96 --size 96 & done\n.\n   Fallback to deep funneled versions for images that dlib failed\n   to align:\n   \n./util/align-dlib.py data/lfw/raw align outerEyesAndNose data/lfw/dlib-affine-sz:96 --size 96 --fallbackLfw data/lfw/deepfunneled\n\n\nGenerate representations with \n./batch-represent/main.lua -outDir evaluation/lfw.nn4.small2.v1.reps -model models/openface/nn4.small2.v1.t7 -data data/lfw/dlib-affine-sz:96\n\n\nGenerate the ROC curve from the \nevaluation\n directory with \n./lfw.py nn4.small2.v1 lfw.nn4.small2.v1.reps\n.\n   This creates \nroc.pdf\n in the \nlfw.nn4.small2.v1.reps\n directory.\n\n\n\n\nProjects with Higher Accuracy\n\n\nIf you're interested in higher accuracy open source code, see:\n\n\nOxford's VGG Face Descriptor\n\n\nThis is licensed for non-commercial research purposes.\nThey've released their softmax network, which obtains .9727 accuracy\non the LFW and will release their triplet network (0.9913 accuracy)\nand data soon (?).\n\n\nTheir softmax model doesn't embed features like FaceNet,\nwhich makes tasks like classification and clustering more difficult.\nTheir triplet model hasn't yet been released, but will provide\nembeddings similar to FaceNet.\nThe triplet model will be supported by OpenFace once it's released.\n\n\nDeep Face Representation\n\n\nThis uses Caffe and doesn't yet have a license.\nThe accuracy on the LFW is .9777.\nThis model doesn't embed features like FaceNet,\nwhich makes tasks like classification and clustering more difficult.",
            "title": "Models and Accuracies"
        },
        {
            "location": "/models-and-accuracies/#models-and-accuracies",
            "text": "This page overviews different OpenFace neural network models\nand is intended for advanced users.",
            "title": "Models and Accuracies"
        },
        {
            "location": "/models-and-accuracies/#model-definitions",
            "text": "The number of parameters are with 128-dimensional embeddings\nand do not include the batch normalization running means and\nvariances.     Model  Number of Parameters      nn4.small2  3733968    nn4.small1  5579520    nn4  6959088    nn2  7472144",
            "title": "Model Definitions"
        },
        {
            "location": "/models-and-accuracies/#pre-trained-models",
            "text": "Models can be trained in different ways with different datasets.\nPre-trained models are versioned and should be released with\na corresponding model definition.\nSwitch between models with caution because the embeddings\nnot compatible with each other.  The current models are trained with a combination of the two largest\n(of August 2015) publicly-available face recognition datasets based on names: FaceScrub \nand  CASIA-WebFace .  The models can be downloaded from our storage servers:   nn4.v1  nn4.v2  nn4.small1.v1  nn4.small2.v1   API differences between the models are:     Model  alignment  landmarkIndices      nn4.v1  openface.AlignDlib.INNER_EYES_AND_BOTTOM_LIP    nn4.v2  openface.AlignDlib.OUTER_EYES_AND_NOSE    nn4.small1.v1  openface.AlignDlib.OUTER_EYES_AND_NOSE    nn4.small2.v1  openface.AlignDlib.OUTER_EYES_AND_NOSE",
            "title": "Pre-trained Models"
        },
        {
            "location": "/models-and-accuracies/#performance",
            "text": "The performance is measured by averaging 500 forward passes with util/profile-network.lua \nand the following results use OpenBLAS on an 8 core 3.70 GHz CPU\nand a Tesla K40 GPU.     Model  Runtime (CPU)  Runtime (GPU)      nn4.v1  75.67 ms \u00b1 19.97 ms  21.96 ms \u00b1 6.71 ms    nn4.v2  82.74 ms \u00b1 19.96 ms  20.82 ms \u00b1 6.03 ms    nn4.small1.v1  69.58 ms \u00b1 16.17 ms  15.90 ms \u00b1 5.18 ms    nn4.small2.v1  58.9 ms \u00b1 15.36 ms  13.72 ms \u00b1 4.64 ms",
            "title": "Performance"
        },
        {
            "location": "/models-and-accuracies/#accuracy-on-the-lfw-benchmark",
            "text": "Even though the public datasets we trained on have orders of magnitude less data\nthan private industry datasets, the accuracy is remarkably high\non the standard LFW \nbenchmark.\nWe had to fallback to using the deep funneled versions for\n58 of 13233 images because dlib failed to detect a face or landmarks.     Model  Accuracy  AUC      nn4.small2.v1  (Default)  0.9292 \u00b1 0.0134  0.973    nn4.small1.v1  0.9210 \u00b1 0.0160  0.973    nn4.v2  0.9157 \u00b1 0.0152  0.966    nn4.v1  0.7612 \u00b1 0.0189  0.853    FaceNet Paper (Reference)  0.9963 \u00b1 0.009  not provided     ROC Curves  nn4.small2.v1   nn4.small1.v1   nn4.v2   nn4.v1",
            "title": "Accuracy on the LFW Benchmark"
        },
        {
            "location": "/models-and-accuracies/#running-the-lfw-experiment",
            "text": "This can be generated with the following commands from the root  openface \ndirectory, assuming you have downloaded and placed the raw and deep funneled \nLFW data from  here \nin  ./data/lfw/raw  and  ./data/lfw/deepfunneled .\nAlso save  pairs.txt  in ./data/lfw/pairs.txt .   Install prerequisites as below.  Preprocess the raw  lfw  images, change  8  to however many\n   separate processes you want to run:\n    for N in {1..8}; do ./util/align-dlib.py data/lfw/raw align outerEyesAndNose data/lfw/dlib-affine-sz:96 --size 96 & done .\n   Fallback to deep funneled versions for images that dlib failed\n   to align:\n    ./util/align-dlib.py data/lfw/raw align outerEyesAndNose data/lfw/dlib-affine-sz:96 --size 96 --fallbackLfw data/lfw/deepfunneled  Generate representations with  ./batch-represent/main.lua -outDir evaluation/lfw.nn4.small2.v1.reps -model models/openface/nn4.small2.v1.t7 -data data/lfw/dlib-affine-sz:96  Generate the ROC curve from the  evaluation  directory with  ./lfw.py nn4.small2.v1 lfw.nn4.small2.v1.reps .\n   This creates  roc.pdf  in the  lfw.nn4.small2.v1.reps  directory.",
            "title": "Running The LFW Experiment"
        },
        {
            "location": "/models-and-accuracies/#projects-with-higher-accuracy",
            "text": "If you're interested in higher accuracy open source code, see:",
            "title": "Projects with Higher Accuracy"
        },
        {
            "location": "/models-and-accuracies/#oxfords-vgg-face-descriptor",
            "text": "This is licensed for non-commercial research purposes.\nThey've released their softmax network, which obtains .9727 accuracy\non the LFW and will release their triplet network (0.9913 accuracy)\nand data soon (?).  Their softmax model doesn't embed features like FaceNet,\nwhich makes tasks like classification and clustering more difficult.\nTheir triplet model hasn't yet been released, but will provide\nembeddings similar to FaceNet.\nThe triplet model will be supported by OpenFace once it's released.",
            "title": "Oxford's VGG Face Descriptor"
        },
        {
            "location": "/models-and-accuracies/#deep-face-representation",
            "text": "This uses Caffe and doesn't yet have a license.\nThe accuracy on the LFW is .9777.\nThis model doesn't embed features like FaceNet,\nwhich makes tasks like classification and clustering more difficult.",
            "title": "Deep Face Representation"
        },
        {
            "location": "/training-new-models/",
            "text": "Training new neural network models\n\n\nWe have also released our deep neural network (DNN)\ntraining infrastructure to promote an open ecosystem and enable quicker\nbootstrapping for new research and development.\n\n\nThere is a distinction between training the DNN model for feature representation\nand training a model for classifying people with the DNN model.\nIf you're interested in creating a new classifier,\nsee \nDemo 3\n.\nThis page is for advanced users interested in training a new DNN model\nand should be done with large datasets (>500k images) to improve the\nfeature representation.\n\n\nWarning:\n Training is computationally and memory expensive and takes a\nday on our Tesla K40 GPU.\n\n\nA rough overview of training is:\n\n\n1. Create raw image directory.\n\n\nCreate a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as \njpg\n or \npng\n and have\na lowercase extension.\n\n\n$ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png\n\n\n\n\n2. Preprocess the raw images\n\n\nIf you plan to compute LFW accuracies, remove all LFW identities for your dataset.\nWe provide an example script doing this with string matching in\n\nremove-lfw-names.py\n.\n\n\nChange \n8\n to however many\nseparate processes you want to run:\n\nfor N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align outerEyesAndNose <path-to-aligned-data> --size 96 & done\n.\n\n\nPrune out directories with less than 3 images per class with\n\n./util/prune-dataset.py <path-to-aligned-data> --numImagesThreshold 3\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Train the model\n\n\nRun \ntraining/main.lua\n to start training the model.\nEdit the dataset options in \ntraining/opts.lua\n or\npass them as command-line parameters.\nThis will output the loss and in-progress models to \ntraining/work\n.\nThe GPU memory usage is determined by the \n-peoplePerBatch\n and\n\n-imagesPerPerson\n parameters, which default to 15 and 20 respectively\nand consume about 12GB of memory.\nThese determine an upper-bound on the mini-batch size and\nshould be reduced for less GPU memory consumption.\n\n\nWarning: Metadata about the on-disk data is cached in\n\ntraining/work/trainCache.t7\n and assumes\nthe data directory does not change.\nIf your data directory changes, delete these\nfiles so they will be regenerated.\n\n\nStopping and starting training\n\n\nModels are saved in the \nwork\n directory after every epoch.\nIf the training process is killed, it can be resumed from\nthe last saved model with the \n-retrain\n option.\nAlso pass a different \n-manualSeed\n so a different image\nsequence is sampled and correctly set \n-epochNumber\n.\n\n\n4. Analyze training\n\n\nVisualize the loss with \ntraining/plot-loss.py\n.\nInstall the Python dependencies from\n\ntraining/requirements.txt\n\nwith \npip2 install -r requirements.txt\n.",
            "title": "Training a DNN Model"
        },
        {
            "location": "/training-new-models/#training-new-neural-network-models",
            "text": "We have also released our deep neural network (DNN)\ntraining infrastructure to promote an open ecosystem and enable quicker\nbootstrapping for new research and development.  There is a distinction between training the DNN model for feature representation\nand training a model for classifying people with the DNN model.\nIf you're interested in creating a new classifier,\nsee  Demo 3 .\nThis page is for advanced users interested in training a new DNN model\nand should be done with large datasets (>500k images) to improve the\nfeature representation.  Warning:  Training is computationally and memory expensive and takes a\nday on our Tesla K40 GPU.  A rough overview of training is:",
            "title": "Training new neural network models"
        },
        {
            "location": "/training-new-models/#1-create-raw-image-directory",
            "text": "Create a directory for your raw images so that images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as  jpg  or  png  and have\na lowercase extension.  $ tree data/mydataset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png",
            "title": "1. Create raw image directory."
        },
        {
            "location": "/training-new-models/#2-preprocess-the-raw-images",
            "text": "If you plan to compute LFW accuracies, remove all LFW identities for your dataset.\nWe provide an example script doing this with string matching in remove-lfw-names.py .  Change  8  to however many\nseparate processes you want to run: for N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align outerEyesAndNose <path-to-aligned-data> --size 96 & done .  Prune out directories with less than 3 images per class with ./util/prune-dataset.py <path-to-aligned-data> --numImagesThreshold 3 .",
            "title": "2. Preprocess the raw images"
        },
        {
            "location": "/training-new-models/#3-train-the-model",
            "text": "Run  training/main.lua  to start training the model.\nEdit the dataset options in  training/opts.lua  or\npass them as command-line parameters.\nThis will output the loss and in-progress models to  training/work .\nThe GPU memory usage is determined by the  -peoplePerBatch  and -imagesPerPerson  parameters, which default to 15 and 20 respectively\nand consume about 12GB of memory.\nThese determine an upper-bound on the mini-batch size and\nshould be reduced for less GPU memory consumption.  Warning: Metadata about the on-disk data is cached in training/work/trainCache.t7  and assumes\nthe data directory does not change.\nIf your data directory changes, delete these\nfiles so they will be regenerated.  Stopping and starting training  Models are saved in the  work  directory after every epoch.\nIf the training process is killed, it can be resumed from\nthe last saved model with the  -retrain  option.\nAlso pass a different  -manualSeed  so a different image\nsequence is sampled and correctly set  -epochNumber .",
            "title": "3. Train the model"
        },
        {
            "location": "/training-new-models/#4-analyze-training",
            "text": "Visualize the loss with  training/plot-loss.py .\nInstall the Python dependencies from training/requirements.txt \nwith  pip2 install -r requirements.txt .",
            "title": "4. Analyze training"
        },
        {
            "location": "/visualizations/",
            "text": "Visualizing representations with t-SNE\n\n\nt-SNE\n is a dimensionality\nreduction technique that can be used to visualize the\n128-dimensional features OpenFace produces.\nThe following shows the visualization of the three people\nin the training and testing dataset with the most images.\n\n\nTraining\n\n\n\n\nTesting\n\n\n\n\nThese can be generated with the following commands from the root\n\nopenface\n directory.\n\n\n1. Create raw image directory.\n\n\nCreate a directory for a subset of raw images that you want to visualize\nwith TSNE.\nMake images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as \njpg\n or \npng\n and have\na lowercase extension.\n\n\n$ tree data/mydataset-subset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png\n\n\n\n\n2. Preprocess the raw images\n\n\nChange \n8\n to however many\nseparate processes you want to run:\n\nfor N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align outerEyesAndNose <path-to-aligned-data> --size 96 & done\n.\n\n\nIf failed alignment attempts causes your directory to have too few images,\nyou can use our utility script\n\n./util/prune-dataset.py\n\nto deletes directories with less than a specified number of images.\n\n\n3. Generate Representations\n\n\n./batch-represent/main.lua -outDir <feature-directory> -data <path-to-aligned-data>\n\ncreates \nreps.csv\n and \nlabels.csv\n in \n<feature-directory>\n.\n\n\n4. Generate TSNE visualization\n\n\nGenerate the t-SNE visualization with\n\n./util/tsne.py <feature-directory> --names <name 1> ... <name n>\n,\nwhere \nname i\n corresponds to label \ni\n from the\nleft-most column in \nlabels.csv\n.\nThis creates \ntsne.pdf\n in \n<feature-directory>\n.\n\n\nVisualizing layer outputs\n\n\nVisualizing the output feature maps of each layer\nis sometimes helpful to understand what features\nthe network has learned to extract.\nWith faces, the locations of the eyes, nose, and\nmouth should play an important role.\n\n\ndemos/vis-outputs.lua\n\noutputs the feature maps from an aligned image.\nThe following shows the first 39 filters of the\nfirst convolutional layer on two images\nof John Lennon.",
            "title": "Visualizations"
        },
        {
            "location": "/visualizations/#visualizing-representations-with-t-sne",
            "text": "t-SNE  is a dimensionality\nreduction technique that can be used to visualize the\n128-dimensional features OpenFace produces.\nThe following shows the visualization of the three people\nin the training and testing dataset with the most images.  Training   Testing   These can be generated with the following commands from the root openface  directory.",
            "title": "Visualizing representations with t-SNE"
        },
        {
            "location": "/visualizations/#1-create-raw-image-directory",
            "text": "Create a directory for a subset of raw images that you want to visualize\nwith TSNE.\nMake images from different\npeople are in different subdirectories. The names of the labels or\nimages do not matter, and each person can have a different amount of images.\nThe images should be formatted as  jpg  or  png  and have\na lowercase extension.  $ tree data/mydataset-subset/raw\nperson-1\n\u251c\u2500\u2500 image-1.jpg\n\u251c\u2500\u2500 image-2.png\n...\n\u2514\u2500\u2500 image-p.png\n\n...\n\nperson-m\n\u251c\u2500\u2500 image-1.png\n\u251c\u2500\u2500 image-2.jpg\n...\n\u2514\u2500\u2500 image-q.png",
            "title": "1. Create raw image directory."
        },
        {
            "location": "/visualizations/#2-preprocess-the-raw-images",
            "text": "Change  8  to however many\nseparate processes you want to run: for N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align outerEyesAndNose <path-to-aligned-data> --size 96 & done .  If failed alignment attempts causes your directory to have too few images,\nyou can use our utility script ./util/prune-dataset.py \nto deletes directories with less than a specified number of images.",
            "title": "2. Preprocess the raw images"
        },
        {
            "location": "/visualizations/#3-generate-representations",
            "text": "./batch-represent/main.lua -outDir <feature-directory> -data <path-to-aligned-data> \ncreates  reps.csv  and  labels.csv  in  <feature-directory> .",
            "title": "3. Generate Representations"
        },
        {
            "location": "/visualizations/#4-generate-tsne-visualization",
            "text": "Generate the t-SNE visualization with ./util/tsne.py <feature-directory> --names <name 1> ... <name n> ,\nwhere  name i  corresponds to label  i  from the\nleft-most column in  labels.csv .\nThis creates  tsne.pdf  in  <feature-directory> .",
            "title": "4. Generate TSNE visualization"
        },
        {
            "location": "/visualizations/#visualizing-layer-outputs",
            "text": "Visualizing the output feature maps of each layer\nis sometimes helpful to understand what features\nthe network has learned to extract.\nWith faces, the locations of the eyes, nose, and\nmouth should play an important role.  demos/vis-outputs.lua \noutputs the feature maps from an aligned image.\nThe following shows the first 39 filters of the\nfirst convolutional layer on two images\nof John Lennon.",
            "title": "Visualizing layer outputs"
        },
        {
            "location": "/release-notes/",
            "text": "Release Notes\n\n\n0.2.1 (2016/01/25)\n\n\n\n\nMinor bug fixes and improved error messages.\n\n\nAdd a LFW classification experiment and an outlier detection script.\n\n\n\n\n0.2.0 (2016/01/19)\n\n\n\n\nSee \nthis blog post\n\n  for an overview and\n  \nthe GitHub Milestone\n\n  for a high-level issue summary.\n\n\nTraining improvements from resulting in an accuracy increase from \n76.1% to 92.9%\n,\n  which are from Bartosz Ludwiczuk's ideas and implementations in\n  \nthis mailing list thread\n.\n  These improvements also reduce the training time from a week to a day.\n\n\nNearly halved execution time thanks to \nHerv\u00e9 Bredin's\n\n  suggestions and sample code for image alignment in\n  \nIssue 50\n.\n\n\nHosted\n  \nPython API Documentation\n.\n\n\nDocker automated build\n online.\n\n\nInitial automatic tests written in \ntests\n.\n\n\nTests successfully passing\n\n  in the Docker automated build in Travis.\n\n\nAdd\n  \nutil/profile-pipeline.py\n\n  to profile the overall execution time on a single image.\n\n\n\n\n0.1.1 (2015/10/15)\n\n\n\n\nFix debug mode of NaiveDlib alignment.\n\n\nAdd\n  \nutil/prune-dataset.py\n\n  for dataset processing.\n\n\nCorrect Docker dependencies.\n\n\n\n\n0.1.0 (2015/10/13)\n\n\n\n\nInitial release.",
            "title": "Release Notes"
        },
        {
            "location": "/release-notes/#release-notes",
            "text": "",
            "title": "Release Notes"
        },
        {
            "location": "/release-notes/#021-20160125",
            "text": "Minor bug fixes and improved error messages.  Add a LFW classification experiment and an outlier detection script.",
            "title": "0.2.1 (2016/01/25)"
        },
        {
            "location": "/release-notes/#020-20160119",
            "text": "See  this blog post \n  for an overview and\n   the GitHub Milestone \n  for a high-level issue summary.  Training improvements from resulting in an accuracy increase from  76.1% to 92.9% ,\n  which are from Bartosz Ludwiczuk's ideas and implementations in\n   this mailing list thread .\n  These improvements also reduce the training time from a week to a day.  Nearly halved execution time thanks to  Herv\u00e9 Bredin's \n  suggestions and sample code for image alignment in\n   Issue 50 .  Hosted\n   Python API Documentation .  Docker automated build  online.  Initial automatic tests written in  tests .  Tests successfully passing \n  in the Docker automated build in Travis.  Add\n   util/profile-pipeline.py \n  to profile the overall execution time on a single image.",
            "title": "0.2.0 (2016/01/19)"
        },
        {
            "location": "/release-notes/#011-20151015",
            "text": "Fix debug mode of NaiveDlib alignment.  Add\n   util/prune-dataset.py \n  for dataset processing.  Correct Docker dependencies.",
            "title": "0.1.1 (2015/10/15)"
        },
        {
            "location": "/release-notes/#010-20151013",
            "text": "Initial release.",
            "title": "0.1.0 (2015/10/13)"
        }
    ]
}